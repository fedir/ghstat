HTTP/1.1 200 OK
Transfer-Encoding: chunked
Access-Control-Allow-Origin: *
Access-Control-Expose-Headers: ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, Deprecation, Sunset
Cache-Control: no-cache
Content-Security-Policy: default-src 'none'
Content-Type: application/json; charset=utf-8
Date: Sat, 14 Nov 2020 21:08:26 GMT
Link: <https://api.github.com/search/issues?q=https%3A%2F%2Fgithub.com%2Fbasho%2Friak+type%3Aissue+state%3Aclosed&page=2>; rel="next", <https://api.github.com/search/issues?q=https%3A%2F%2Fgithub.com%2Fbasho%2Friak+type%3Aissue+state%3Aclosed&page=34>; rel="last"
Referrer-Policy: origin-when-cross-origin, strict-origin-when-cross-origin
Server: GitHub.com
Status: 200 OK
Strict-Transport-Security: max-age=31536000; includeSubdomains; preload
Vary: Accept, Authorization, Cookie, X-GitHub-OTP
Vary: Accept-Encoding, Accept, X-Requested-With
Vary: Accept-Encoding
X-Accepted-Oauth-Scopes: 
X-Content-Type-Options: nosniff
X-Frame-Options: deny
X-Github-Media-Type: github.v3; format=json
X-Github-Request-Id: D8E5:5C53:43806E9:4DCCF7D:5FB04749
X-Oauth-Scopes: repo
X-Ratelimit-Limit: 30
X-Ratelimit-Remaining: 21
X-Ratelimit-Reset: 1605388137
X-Ratelimit-Used: 9
X-Xss-Protection: 1; mode=block

26d44
{"total_count":1351,"incomplete_results":false,"items":[{"url":"https://api.github.com/repos/basho/riak_kv/issues/1758","repository_url":"https://api.github.com/repos/basho/riak_kv","labels_url":"https://api.github.com/repos/basho/riak_kv/issues/1758/labels{/name}","comments_url":"https://api.github.com/repos/basho/riak_kv/issues/1758/comments","events_url":"https://api.github.com/repos/basho/riak_kv/issues/1758/events","html_url":"https://github.com/basho/riak_kv/issues/1758","id":632703648,"node_id":"MDU6SXNzdWU2MzI3MDM2NDg=","number":1758,"title":"Unhandled false return from riak_index:object_key_in_range","user":{"login":"pma","id":37843,"node_id":"MDQ6VXNlcjM3ODQz","avatar_url":"https://avatars3.githubusercontent.com/u/37843?v=4","gravatar_id":"","url":"https://api.github.com/users/pma","html_url":"https://github.com/pma","followers_url":"https://api.github.com/users/pma/followers","following_url":"https://api.github.com/users/pma/following{/other_user}","gists_url":"https://api.github.com/users/pma/gists{/gist_id}","starred_url":"https://api.github.com/users/pma/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pma/subscriptions","organizations_url":"https://api.github.com/users/pma/orgs","repos_url":"https://api.github.com/users/pma/repos","events_url":"https://api.github.com/users/pma/events{/privacy}","received_events_url":"https://api.github.com/users/pma/received_events","type":"User","site_admin":false},"labels":[{"id":1373832059,"node_id":"MDU6TGFiZWwxMzczODMyMDU5","url":"https://api.github.com/repos/basho/riak_kv/labels/2.9%20Known%20Issue","name":"2.9 Known Issue","color":"e87fd4","default":false,"description":"Known issues with the Riak 2.9 release"}],"state":"closed","locked":false,"assignee":{"login":"martinsumner","id":1628897,"node_id":"MDQ6VXNlcjE2Mjg4OTc=","avatar_url":"https://avatars0.githubusercontent.com/u/1628897?v=4","gravatar_id":"","url":"https://api.github.com/users/martinsumner","html_url":"https://github.com/martinsumner","followers_url":"https://api.github.com/users/martinsumner/followers","following_url":"https://api.github.com/users/martinsumner/following{/other_user}","gists_url":"https://api.github.com/users/martinsumner/gists{/gist_id}","starred_url":"https://api.github.com/users/martinsumner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/martinsumner/subscriptions","organizations_url":"https://api.github.com/users/martinsumner/orgs","repos_url":"https://api.github.com/users/martinsumner/repos","events_url":"https://api.github.com/users/martinsumner/events{/privacy}","received_events_url":"https://api.github.com/users/martinsumner/received_events","type":"User","site_admin":false},"assignees":[{"login":"martinsumner","id":1628897,"node_id":"MDQ6VXNlcjE2Mjg4OTc=","avatar_url":"https://avatars0.githubusercontent.com/u/1628897?v=4","gravatar_id":"","url":"https://api.github.com/users/martinsumner","html_url":"https://github.com/martinsumner","followers_url":"https://api.github.com/users/martinsumner/followers","following_url":"https://api.github.com/users/martinsumner/following{/other_user}","gists_url":"https://api.github.com/users/martinsumner/gists{/gist_id}","starred_url":"https://api.github.com/users/martinsumner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/martinsumner/subscriptions","organizations_url":"https://api.github.com/users/martinsumner/orgs","repos_url":"https://api.github.com/users/martinsumner/repos","events_url":"https://api.github.com/users/martinsumner/events{/privacy}","received_events_url":"https://api.github.com/users/martinsumner/received_events","type":"User","site_admin":false}],"milestone":null,"comments":5,"created_at":"2020-06-06T20:17:19Z","updated_at":"2020-09-01T08:43:48Z","closed_at":"2020-09-01T08:43:48Z","author_association":"NONE","active_lock_reason":null,"body":"This case statement https://github.com/basho/riak_kv/blob/develop-2.9/src/riak_kv_leveled_backend.erl#L405 should handle a possible return of 'false' from riak_index:object_key_in_range, due to this line here: \r\nhttps://github.com/basho/riak_kv/blob/develop-2.9/src/riak_index.erl#L469\r\n\r\nI got the pattern match error while playing with RiakCS using leveled as backend. Listing directories via the s3 API would trigger the error under some situations I couldn't  reproduce reliably.","performed_via_github_app":null,"score":1.0},{"url":"https://api.github.com/repos/basho/riak_kv/issues/1759","repository_url":"https://api.github.com/repos/basho/riak_kv","labels_url":"https://api.github.com/repos/basho/riak_kv/issues/1759/labels{/name}","comments_url":"https://api.github.com/repos/basho/riak_kv/issues/1759/comments","events_url":"https://api.github.com/repos/basho/riak_kv/issues/1759/events","html_url":"https://github.com/basho/riak_kv/issues/1759","id":634535807,"node_id":"MDU6SXNzdWU2MzQ1MzU4MDc=","number":1759,"title":"Vnode clearing does not clear tictac_aae state","user":{"login":"martinsumner","id":1628897,"node_id":"MDQ6VXNlcjE2Mjg4OTc=","avatar_url":"https://avatars0.githubusercontent.com/u/1628897?v=4","gravatar_id":"","url":"https://api.github.com/users/martinsumner","html_url":"https://github.com/martinsumner","followers_url":"https://api.github.com/users/martinsumner/followers","following_url":"https://api.github.com/users/martinsumner/following{/other_user}","gists_url":"https://api.github.com/users/martinsumner/gists{/gist_id}","starred_url":"https://api.github.com/users/martinsumner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/martinsumner/subscriptions","organizations_url":"https://api.github.com/users/martinsumner/orgs","repos_url":"https://api.github.com/users/martinsumner/repos","events_url":"https://api.github.com/users/martinsumner/events{/privacy}","received_events_url":"https://api.github.com/users/martinsumner/received_events","type":"User","site_admin":false},"labels":[{"id":1373832059,"node_id":"MDU6TGFiZWwxMzczODMyMDU5","url":"https://api.github.com/repos/basho/riak_kv/labels/2.9%20Known%20Issue","name":"2.9 Known Issue","color":"e87fd4","default":false,"description":"Known issues with the Riak 2.9 release"}],"state":"closed","locked":false,"assignee":{"login":"martinsumner","id":1628897,"node_id":"MDQ6VXNlcjE2Mjg4OTc=","avatar_url":"https://avatars0.githubusercontent.com/u/1628897?v=4","gravatar_id":"","url":"https://api.github.com/users/martinsumner","html_url":"https://github.com/martinsumner","followers_url":"https://api.github.com/users/martinsumner/followers","following_url":"https://api.github.com/users/martinsumner/following{/other_user}","gists_url":"https://api.github.com/users/martinsumner/gists{/gist_id}","starred_url":"https://api.github.com/users/martinsumner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/martinsumner/subscriptions","organizations_url":"https://api.github.com/users/martinsumner/orgs","repos_url":"https://api.github.com/users/martinsumner/repos","events_url":"https://api.github.com/users/martinsumner/events{/privacy}","received_events_url":"https://api.github.com/users/martinsumner/received_events","type":"User","site_admin":false},"assignees":[{"login":"martinsumner","id":1628897,"node_id":"MDQ6VXNlcjE2Mjg4OTc=","avatar_url":"https://avatars0.githubusercontent.com/u/1628897?v=4","gravatar_id":"","url":"https://api.github.com/users/martinsumner","html_url":"https://github.com/martinsumner","followers_url":"https://api.github.com/users/martinsumner/followers","following_url":"https://api.github.com/users/martinsumner/following{/other_user}","gists_url":"https://api.github.com/users/martinsumner/gists{/gist_id}","starred_url":"https://api.github.com/users/martinsumner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/martinsumner/subscriptions","organizations_url":"https://api.github.com/users/martinsumner/orgs","repos_url":"https://api.github.com/users/martinsumner/repos","events_url":"https://api.github.com/users/martinsumner/events{/privacy}","received_events_url":"https://api.github.com/users/martinsumner/received_events","type":"User","site_admin":false}],"milestone":null,"comments":0,"created_at":"2020-06-08T11:41:46Z","updated_at":"2020-09-01T08:43:20Z","closed_at":"2020-09-01T08:43:20Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"https://github.com/basho/riak_kv/blob/develop-2.9/src/riak_kv_vnode.erl#L2208-L2225\r\n\r\nThis does not destroy any parallel tictac_aae store, or tictac_aae hashtree caches.  This can lead to problems when a node leaves, then is re-joined without manually clearing the disk.","performed_via_github_app":null,"score":1.0},{"url":"https://api.github.com/repos/basho/riak_core/issues/958","repository_url":"https://api.github.com/repos/basho/riak_core","labels_url":"https://api.github.com/repos/basho/riak_core/issues/958/labels{/name}","comments_url":"https://api.github.com/repos/basho/riak_core/issues/958/comments","events_url":"https://api.github.com/repos/basho/riak_core/issues/958/events","html_url":"https://github.com/basho/riak_core/issues/958","id":608149889,"node_id":"MDU6SXNzdWU2MDgxNDk4ODk=","number":958,"title":"Deadlock reappears in riak_core_worker_pool.","user":{"login":"rjmh","id":1872017,"node_id":"MDQ6VXNlcjE4NzIwMTc=","avatar_url":"https://avatars2.githubusercontent.com/u/1872017?v=4","gravatar_id":"","url":"https://api.github.com/users/rjmh","html_url":"https://github.com/rjmh","followers_url":"https://api.github.com/users/rjmh/followers","following_url":"https://api.github.com/users/rjmh/following{/other_user}","gists_url":"https://api.github.com/users/rjmh/gists{/gist_id}","starred_url":"https://api.github.com/users/rjmh/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rjmh/subscriptions","organizations_url":"https://api.github.com/users/rjmh/orgs","repos_url":"https://api.github.com/users/rjmh/repos","events_url":"https://api.github.com/users/rjmh/events{/privacy}","received_events_url":"https://api.github.com/users/rjmh/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2020-04-28T09:06:07Z","updated_at":"2020-08-26T15:36:30Z","closed_at":"2020-08-05T22:22:06Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"An old deadlock has reappeared in riak_core_worker_pool. The original problem and its fix are described here (from 2013):\r\n\r\n[https://github.com/basho/riak_core/issues/298](https://github.com/basho/riak_core/issues/298)\r\n\r\nIt appears that the fix didn't entirely solve the problem.\r\n\r\nThe problem is demonstrated by `worker_pool_pulse`, in the case when the worker pool is of size 1, and the work to be done is `[0,die,0]` (three tasks, of which the second one dies). For this test case, about one in 16,000 seeds leads to the bug manifesting; it can be provoked every time by supplying the right seed, as follows.\r\n\r\n```\r\ntest_deadlock() ->\r\n    eqc:check(prop_any_pool(),[{{77264,21542,12323},true,[0,die,0]}]).\r\n```\r\n\r\nBriefly, the RCWP uses poolboy to create its workers, but recycles workers for new tasks without passing them back through poolboy. It has two main states, 'ready' and 'queueing'. In the ready state, it asks poolboy for a new worker when new work arrives, until poolboy replies 'full', when it queues the work and enters the queueing state. In the queueing state, new work is always queued, and jobs are handed out to workers from the queue as they check back in (when a job is finished). The RCWP returns to the ready state only when a worker checks in, but there are no jobs in the queue, so the worker is returned to poolboy. RCWP thus **remains in the queueing state, even if the queue is empty**.\r\n\r\nWhen a worker dies, it will never check back in to RCWP, but poolboy will notice and replace it. The new worker sends `worker_start` to RCWP, which then checks the worker out from poolboy and gives it a job--**if there is work waiting to be done**. But if the queue is empty, then RCWP just stays in the same state, and will never ask poolboy for the replacement worker unless another worker checks in while the queue is empty (prompting a return to the ready state). If the pool size is one, then RCWP deadlocks the first time this happens. I believe it could also happen repeatedly, if the work load is high enough that RCWP never returns to the ready state--every time a worker crashes while the queue is empty, that worker will be lost forever, leading to deadlock eventually.\r\n\r\nHere is the PULSE visualization of the deadlock. The thing is in the RCWP process on the left; in the last two steps, the worker_start message arrives before the next work.\r\n\r\n![graph1](https://user-images.githubusercontent.com/1872017/80468836-026f9200-8940-11ea-9124-97a1219c7225.jpg)\r\n","performed_via_github_app":null,"score":1.0},{"url":"https://api.github.com/repos/basho/riak_kv/issues/1661","repository_url":"https://api.github.com/repos/basho/riak_kv","labels_url":"https://api.github.com/repos/basho/riak_kv/issues/1661/labels{/name}","comments_url":"https://api.github.com/repos/basho/riak_kv/issues/1661/comments","events_url":"https://api.github.com/repos/basho/riak_kv/issues/1661/events","html_url":"https://github.com/basho/riak_kv/issues/1661","id":294722867,"node_id":"MDU6SXNzdWUyOTQ3MjI4Njc=","number":1661,"title":"Soft threshold checks for coordinators","user":{"login":"martinsumner","id":1628897,"node_id":"MDQ6VXNlcjE2Mjg4OTc=","avatar_url":"https://avatars0.githubusercontent.com/u/1628897?v=4","gravatar_id":"","url":"https://api.github.com/users/martinsumner","html_url":"https://github.com/martinsumner","followers_url":"https://api.github.com/users/martinsumner/followers","following_url":"https://api.github.com/users/martinsumner/following{/other_user}","gists_url":"https://api.github.com/users/martinsumner/gists{/gist_id}","starred_url":"https://api.github.com/users/martinsumner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/martinsumner/subscriptions","organizations_url":"https://api.github.com/users/martinsumner/orgs","repos_url":"https://api.github.com/users/martinsumner/repos","events_url":"https://api.github.com/users/martinsumner/events{/privacy}","received_events_url":"https://api.github.com/users/martinsumner/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":10,"created_at":"2018-02-06T11:20:40Z","updated_at":"2020-09-01T08:52:01Z","closed_at":"2020-09-01T08:52:01Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"## Background\r\n\r\nRiak for most operations does a good job of working around the slow node problem.  In sending n requests for a GET and being able to return a response on r replies, low variance in client response times can still be achieved even when nodes are impaired by hardware failure (e.g. loss of disk in a RAID array) or software interruption (e.g AAE tree rebuilds, page cache pollution).  \r\n\r\nRiak does not necessarily deliver a low median response time, but it does deliver a relatively low maximum/high-percentile response time.\r\n\r\nThis property does not necessarily hold though when an operation requires a coordinator, for example PUT operations.  When a coordinator is selected for a PUT, if the coordinating vnode is on a slow node, or for vnode specific reasons has a large mailbox queue - the PUT will run at the pace of that vnode.  In testing at the NHS under certain high pressure scenarios significant increases in mean PUT response times have been seen, driven by a sharp increase in slow outliers, as vnode mailbox queues rise, and those vnodes take their equal share of chances to be the PUT coordinator.  \r\n\r\nThese circumstances can be triggered by big events (such as the failures and expensive processes previously referred to), but also by relatively hard to detect events (larger than usual compaction activity in a vnode, a node acting as coordinator for an exceptionally large index query etc).  Running a cluster with disk utilisation at around 85% will lead to frequent occurrences of bursts of very slow PUTs due to short-term fluctuations between nodes in the vnode mailbox queue size, with bursts where queue sizes across a node exceed 100 messages.\r\n\r\nAlthough only PUT currently has a coordinator, there is a proposal from @martincox to use coordinators for GETs as well.  This is to allow Riak to avoid unnecessarily doing n GETs, and instead do 1 GET and n-1 HEAD requests (in most scenarios) - reducing both the overall vnode load and network bandwidth used, especially when values are large.  To implement this though, a vnode must be nominated by the GET FSM to be the one responsible for the GET and not the HEAD (and the response to the client will be at least as slow as this single vnode (as with the coordinated PUT).\r\n\r\nIt would be preferable if this efficiency change could be made without running the risk of seeing response time volatility as individual vnodes experience increased queue lengths, for example due to additional background workload on that node.  Previously it had been suggested that this could be avoided by using n-HEADs followed by 1-GET (https://github.com/martinsumner/leveled/blob/master/docs/FUTURE.md#n-heads-1-get-or-1-get-n-1-heads), but this requires an additional backend message, and does nothing to directly help the PUT scenario (other than through indirectly reducing the risk of larger mailbox queues by diverting more expensive GET operations from vnodes with backlogs).\r\n\r\n## Proposal\r\n\r\nIt is proposed that the general problem of selecting 1 out of n vnodes for extra work, i.e. selecting a coordinator), could be addressed by using soft threshold mailbox queue checks to the vnode proxy.  These checks will be \"soft\" in comparison to the current \"hard\" overload checks made at the vnode proxy.  The checks will allow for the choice of coordinator to be validated before it is used, at a very low cost, with no additional backend work required.  \r\n\r\n## Mailbox Monitoring\r\n\r\nCurrently all messages to vnodes pass through the riak_core_vnode_proxy (https://github.com/basho/riak_core/blob/2.1.9/src/riak_core_vnode_proxy.erl).\r\n\r\nThis proxy monitors the mailbox queue size (very loosely) in order to detect the overload state (https://github.com/basho/riak_core/blob/2.1.9/src/riak_core_vnode_proxy.erl#L277-L288).  This state, effectively fails all messages received once the mailbox has reached the default threshold of 10000 messages.  [Note that handle_overload_command is not part of the required vnode behaviour, but is implemented in riak_kv_vnode to return an error].  The default threshold here is probably two orders of magnitude too large for this purpose, and the consequent action unhelpful. \r\n\r\nThe proposal is to monitor the mailbox size more accurately within the vnode_proxy, so that the proxy can handle a soft_overload ping message responding with the approximation of the current mailbox size.   This could be achieved by using the same polling algorithm with a lower request interval (https://github.com/basho/riak_core/blob/2.1.9/src/riak_core_vnode_proxy.erl#L240), without reducing significantly the interval for the expensive/accurate check (https://github.com/basho/riak_core/blob/2.1.9/src/riak_core_vnode_proxy.erl#L254).  A request interval of 50 is perhaps a good starting point.\r\n\r\nThe vnode_proxy could then receive a new queue_size message, that would return the current queue estimate, or `false` if the current queue estimate is less than twice the poll frequency (100 if a request interval of 50 is used by default).  This queue estimate could then be used when selecting coordinators.\r\n\r\n## Selecting A Coordinator \r\n\r\nThe vnode_proxy queue_size message can offer an immediate response from the proxy LoopState without forwarding to the backend impacting any backlog there.  So if a coordinator is chosen by a PUT or GET FSM, with presumably local vnodes in the preflist chosen first as at present - a queue_size message may be sent first to see if the current mailbox queue size is below the threshold (i.e. `false` is returned).  If the queue size is above the threshold other vnodes may be polled for queue_size and the shortest queue chosen.\r\n\r\nThis queue_size request could be made for every coordinated request.  Alternatively, a cache of queue size responses for each node could be kept in a name referenced ETS table, and a poll only be used if the cached entry in the table for the chosen vnode has expired.  Given that the check is so low cost, perhaps sending one message for every coordinator nomination is acceptable.\r\n\r\nFSMs waiting for a queue_size message should do so with an aggressive timeout (e.g. 1s), and assume the queue size is currently infinite if the timeout is hit without a response to the queue_size message.  If no vnodes can return a queue_size within the timeout, then this should be treated as an overload error.\r\n\r\nIn the current PUT FSM implementation it would probably be preferable to make the queue_size check before making the forwarding decision: i.e. currently the PUT FSM chooses not to forward is a vnode in the preflist is local, and forward to the first vnode in the preflist otherwise.","performed_via_github_app":null,"score":1.0},{"url":"https://api.github.com/repos/apache/trafficcontrol/issues/4945","repository_url":"https://api.github.com/repos/apache/trafficcontrol","labels_url":"https://api.github.com/repos/apache/trafficcontrol/issues/4945/labels{/name}","comments_url":"https://api.github.com/repos/apache/trafficcontrol/issues/4945/comments","events_url":"https://api.github.com/repos/apache/trafficcontrol/issues/4945/events","html_url":"https://github.com/apache/trafficcontrol/issues/4945","id":677779758,"node_id":"MDU6SXNzdWU2Nzc3Nzk3NTg=","number":4945,"title":"Traffic Ops tests fail with: ats/atscdn/headerrewritedotconfig.go:1:1: expected 'package', found 'EOF'","user":{"login":"zrhoffman","id":11163823,"node_id":"MDQ6VXNlcjExMTYzODIz","avatar_url":"https://avatars0.githubusercontent.com/u/11163823?v=4","gravatar_id":"","url":"https://api.github.com/users/zrhoffman","html_url":"https://github.com/zrhoffman","followers_url":"https://api.github.com/users/zrhoffman/followers","following_url":"https://api.github.com/users/zrhoffman/following{/other_user}","gists_url":"https://api.github.com/users/zrhoffman/gists{/gist_id}","starred_url":"https://api.github.com/users/zrhoffman/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zrhoffman/subscriptions","organizations_url":"https://api.github.com/users/zrhoffman/orgs","repos_url":"https://api.github.com/users/zrhoffman/repos","events_url":"https://api.github.com/users/zrhoffman/events{/privacy}","received_events_url":"https://api.github.com/users/zrhoffman/received_events","type":"User","site_admin":false},"labels":[{"id":675814919,"node_id":"MDU6TGFiZWw2NzU4MTQ5MTk=","url":"https://api.github.com/repos/apache/trafficcontrol/labels/Traffic%20Ops","name":"Traffic Ops","color":"808080","default":false,"description":"related to Traffic Ops"},{"id":435396616,"node_id":"MDU6TGFiZWw0MzUzOTY2MTY=","url":"https://api.github.com/repos/apache/trafficcontrol/labels/bug","name":"bug","color":"ee0701","default":true,"description":"something isn't working as intended"},{"id":906177679,"node_id":"MDU6TGFiZWw5MDYxNzc2Nzk=","url":"https://api.github.com/repos/apache/trafficcontrol/labels/tests","name":"tests","color":"0052cc","default":false,"description":"related to tests and/or testing infrastructure"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2020-08-12T15:21:40Z","updated_at":"2020-08-12T17:14:14Z","closed_at":"2020-08-12T17:14:14Z","author_association":"MEMBER","active_lock_reason":null,"body":"<!--\r\n************ STOP!! ************\r\nIf this issue identifies a security vulnerability, DO NOT submit it! Instead, contact\r\nthe Apache Software Foundation Security Team at security@trafficcontrol.apache.org and follow the\r\nguidelines at https: //www.apache.org/security/ regarding vulnerability disclosure.\r\n\r\n- For *SUPPORT QUESTIONS*, use the\r\n[Traffic Control slack channels](https: //traffic-control-cdn.slack.com) or [Traffic Control mailing lists](http://trafficcontrol.apache.org/mailing_lists/).\r\n- Before submitting, please **SEARCH GITHUB** for a similar issue or PR.-->\r\n\r\n## I'm submitting a ...\r\n<!-- delete all those that don't apply -->\r\n<!--- security vulnerability (STOP!! - see above)-->\r\n-  bug report\r\n\r\n## Traffic Control components affected ...\r\n<!-- delete all those that don't apply -->\r\n-  Traffic Ops\r\n\r\n## Current behavior:\r\n<!-- Describe how the bug manifests / how the current features are insufficient.-->\r\nTraffic Ops unit tests fail with\r\n\r\n```\r\nats/atscdn/headerrewritedotconfig.go:1:1: expected 'package', found 'EOF'\r\n```\r\n\r\n## Expected / new behavior:\r\n<!-- Describe what the behavior would be without the bug / how the feature would improve Traffic Control -->\r\nTraffic Ops unit tests should pass\r\n\r\n## Minimal reproduction of the problem with instructions:\r\n<!--\r\nIf the current behavior is a bug or you can illustrate your feature request better with an example,\r\nplease provide the *STEPS TO REPRODUCE* and include the applicable TC version.\r\n-->\r\ncd traffic_ops/app/bin/tests\r\ndocker-compose up --build\r\n\r\n## Anything else:\r\n<!-- e.g.stacktraces, related issues, suggestions how to fix -->\r\n<details><summary>Logs for the above unit tests (click to expand)</summary>\r\n\r\n```docker-compose\r\nNative build is an experimental feature and could change at any time\r\nBuilding unit_golang\r\n#1 [internal] load build definition from Dockerfile-golangtest\r\n#1 transferring dockerfile: 49B done\r\n#1 DONE 0.1s\r\n\r\n#2 [internal] load .dockerignore\r\n#2 transferring context: 2B done\r\n#2 DONE 0.2s\r\n\r\n#3 [internal] load metadata for docker.io/library/golang:1.14.2\r\n#3 DONE 0.7s\r\n\r\n#4 [1/6] FROM docker.io/library/golang:1.14.2@sha256:1bd634c5a72bfa7fb48d54...\r\n#4 DONE 0.0s\r\n\r\n#5 [internal] load build context\r\n#5 transferring context: 1.34MB 0.3s done\r\n#5 DONE 0.4s\r\n\r\n#4 [1/6] FROM docker.io/library/golang:1.14.2@sha256:1bd634c5a72bfa7fb48d54...\r\n#4 CACHED\r\n\r\n#6 [2/6] ADD traffic_ops /go/src/github.com/apache/trafficcontrol/traffic_o...\r\n#6 DONE 3.2s\r\n\r\n#7 [3/6] ADD lib /go/src/github.com/apache/trafficcontrol/lib\r\n#7 DONE 0.2s\r\n\r\n#8 [4/6] ADD traffic_monitor /go/src/github.com/apache/trafficcontrol/traff...\r\n#8 DONE 0.2s\r\n\r\n#9 [5/6] ADD vendor /go/src/github.com/apache/trafficcontrol/vendor\r\n#9 DONE 0.3s\r\n\r\n#10 [6/6] WORKDIR /go/src/github.com/apache/trafficcontrol/traffic_ops/traff...\r\n#10 DONE 0.1s\r\n\r\n#11 exporting to image\r\n#11 exporting layers\r\n#11 exporting layers 4.5s done\r\n#11 writing image sha256:a6a64f9579cd9e3e74eb6313ae9f6aca829973fa956db4008b5001673ebf93df done\r\n#11 naming to docker.io/library/tests_unit_golang done\r\n#11 DONE 4.6s\r\nSuccessfully built a6a64f9579cd9e3e74eb6313ae9f6aca829973fa956db4008b5001673ebf93df\r\nStarting tests_db_1 ... \r\nRecreating tests_unit_golang_1 ... \r\nStarting tests_db_1            ... done\r\nRecreating tests_unit_golang_1 ... done\r\nAttaching to tests_db_1, tests_unit_golang_1\r\ndb_1           | Error: Database is uninitialized and superuser password is not specified.\r\ndb_1           |        You must specify POSTGRES_PASSWORD to a non-empty value for the\r\ndb_1           |        superuser. For example, \"-e POSTGRES_PASSWORD=password\" on \"docker run\".\r\ndb_1           | \r\ndb_1           |        You may also use \"POSTGRES_HOST_AUTH_METHOD=trust\" to allow all\r\ndb_1           |        connections without a password. This is *not* recommended.\r\ndb_1           | \r\ndb_1           |        See PostgreSQL documentation about \"trust\":\r\ndb_1           |        https://www.postgresql.org/docs/current/auth-trust.html\r\nunit_golang_1  | get \"golang.org/x/crypto/scrypt\": found meta tag get.metaImport{Prefix:\"golang.org/x/crypto\", VCS:\"git\", RepoRoot:\"https://go.googlesource.com/crypto\"} at //golang.org/x/crypto/scrypt?go-get=1\r\nunit_golang_1  | get \"golang.org/x/crypto/scrypt\": verifying non-authoritative meta tag\r\nunit_golang_1  | golang.org/x/crypto (download)\r\ntests_db_1 exited with code 1\r\nunit_golang_1  | get \"golang.org/x/net/publicsuffix\": found meta tag get.metaImport{Prefix:\"golang.org/x/net\", VCS:\"git\", RepoRoot:\"https://go.googlesource.com/net\"} at //golang.org/x/net/publicsuffix?go-get=1\r\nunit_golang_1  | get \"golang.org/x/net/publicsuffix\": verifying non-authoritative meta tag\r\nunit_golang_1  | golang.org/x/net (download)\r\nunit_golang_1  | get \"golang.org/x/crypto/ed25519\": found meta tag get.metaImport{Prefix:\"golang.org/x/crypto\", VCS:\"git\", RepoRoot:\"https://go.googlesource.com/crypto\"} at //golang.org/x/crypto/ed25519?go-get=1\r\nunit_golang_1  | get \"golang.org/x/crypto/ed25519\": verifying non-authoritative meta tag\r\nunit_golang_1  | get \"golang.org/x/net/ipv4\": found meta tag get.metaImport{Prefix:\"golang.org/x/net\", VCS:\"git\", RepoRoot:\"https://go.googlesource.com/net\"} at //golang.org/x/net/ipv4?go-get=1\r\nunit_golang_1  | get \"golang.org/x/net/ipv4\": verifying non-authoritative meta tag\r\nunit_golang_1  | get \"golang.org/x/sys/unix\": found meta tag get.metaImport{Prefix:\"golang.org/x/sys\", VCS:\"git\", RepoRoot:\"https://go.googlesource.com/sys\"} at //golang.org/x/sys/unix?go-get=1\r\nunit_golang_1  | get \"golang.org/x/sys/unix\": verifying non-authoritative meta tag\r\nunit_golang_1  | golang.org/x/sys (download)\r\nunit_golang_1  | get \"golang.org/x/net/ipv6\": found meta tag get.metaImport{Prefix:\"golang.org/x/net\", VCS:\"git\", RepoRoot:\"https://go.googlesource.com/net\"} at //golang.org/x/net/ipv6?go-get=1\r\nunit_golang_1  | get \"golang.org/x/net/ipv6\": verifying non-authoritative meta tag\r\nunit_golang_1  | github.com/apache/trafficcontrol/vendor/github.com/lib/pq/oid\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/vendor/github.com/go-acme/lego/vendor/golang.org/x/net/internal/iana\r\nunit_golang_1  | golang.org/x/net/internal/iana\r\nunit_golang_1  | golang.org/x/sys/internal/unsafeheader\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/vendor/github.com/GehirnInc/crypt/internal\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/vendor/github.com/lestrrat-go/jwx/internal/option\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/vendor/github.com/lestrrat/go-jwx/jwa\r\nunit_golang_1  | golang.org/x/crypto/pbkdf2\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/vendor/github.com/go-acme/lego/vendor/golang.org/x/crypto/pbkdf2\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/vendor/github.com/go-acme/lego/vendor/golang.org/x/text/transform\r\nunit_golang_1  | github.com/apache/trafficcontrol/vendor/github.com/influxdata/influxdb/pkg/escape\r\nunit_golang_1  | github.com/apache/trafficcontrol/vendor/github.com/jmoiron/sqlx/reflectx\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/vendor/github.com/basho/backoff\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/vendor/github.com/go-acme/lego/vendor/github.com/cenkalti/backoff\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/vendor/github.com/go-acme/lego/vendor/golang.org/x/crypto/ed25519/internal/edwards25519\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/vendor/github.com/go-acme/lego/vendor/golang.org/x/sys/unix\r\nunit_golang_1  | golang.org/x/sys/unix\r\nunit_golang_1  | github.com/apache/trafficcontrol/lib/go-log\r\nunit_golang_1  | github.com/apache/trafficcontrol/vendor/github.com/go-ozzo/ozzo-validation\r\nunit_golang_1  | github.com/apache/trafficcontrol/vendor/github.com/asaskevich/govalidator\r\nunit_golang_1  | github.com/apache/trafficcontrol/vendor/github.com/lib/pq\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/about\r\nunit_golang_1  | github.com/apache/trafficcontrol/vendor/github.com/influxdata/influxdb/models\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/vendor/github.com/golang/protobuf/proto\r\nunit_golang_1  | github.com/apache/trafficcontrol/lib/go-util\r\nunit_golang_1  | golang.org/x/crypto/scrypt\r\nunit_golang_1  | github.com/apache/trafficcontrol/vendor/github.com/jmoiron/sqlx\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/vendor/gopkg.in/asn1-ber.v1\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/tocookie\r\nunit_golang_1  | github.com/apache/trafficcontrol/lib/go-rfc\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_monitor/vendor/github.com/hydrogen18/stoppableListener\r\nunit_golang_1  | github.com/apache/trafficcontrol/vendor/github.com/modern-go/concurrent\r\nunit_golang_1  | golang.org/x/net/publicsuffix\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/vendor/github.com/go-acme/lego/vendor/golang.org/x/crypto/ocsp\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/vendor/gopkg.in/ldap.v2\r\nunit_golang_1  | github.com/apache/trafficcontrol/vendor/github.com/influxdata/influxdb/client/v2\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_monitor/srvhttp\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/vendor/github.com/go-acme/lego/acme\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/vendor/github.com/go-acme/lego/certcrypto\r\nunit_golang_1  | github.com/apache/trafficcontrol/vendor/github.com/modern-go/reflect2\r\nunit_golang_1  | github.com/apache/trafficcontrol/vendor/github.com/go-ozzo/ozzo-validation/is\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/vendor/github.com/go-acme/lego/acme/api/internal/sender\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/vendor/github.com/go-acme/lego/vendor/golang.org/x/crypto/ed25519\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/vendor/github.com/go-acme/lego/vendor/gopkg.in/square/go-jose.v2/cipher\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/vendor/github.com/go-acme/lego/vendor/gopkg.in/square/go-jose.v2/json\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/vendor/github.com/go-acme/lego/acme/api/internal/nonces\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/vendor/github.com/go-acme/lego/log\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/vendor/github.com/go-acme/lego/challenge\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/vendor/github.com/go-acme/lego/vendor/golang.org/x/text/unicode/bidi\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/vendor/github.com/go-acme/lego/vendor/golang.org/x/text/unicode/norm\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/vendor/github.com/go-acme/lego/vendor/golang.org/x/net/bpf\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/vendor/github.com/go-acme/lego/platform/wait\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/vendor/github.com/go-acme/lego/vendor/golang.org/x/net/internal/socket\r\nunit_golang_1  | github.com/apache/trafficcontrol/lib/go-tc/tovalidate\r\nunit_golang_1  | golang.org/x/crypto/ed25519\r\nunit_golang_1  | golang.org/x/net/bpf\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/vendor/github.com/GehirnInc/crypt/common\r\nunit_golang_1  | golang.org/x/net/internal/socket\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/vendor/github.com/GehirnInc/crypt\r\nunit_golang_1  | github.com/apache/trafficcontrol/lib/go-tc\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/vendor/github.com/dgrijalva/jwt-go\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/vendor/github.com/go-acme/lego/vendor/golang.org/x/text/secure/bidirule\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/vendor/github.com/go-acme/lego/vendor/golang.org/x/net/ipv4\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/vendor/github.com/go-acme/lego/vendor/golang.org/x/net/ipv6\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/vendor/github.com/GehirnInc/crypt/md5_crypt\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/vendor/github.com/lestrrat-go/jwx/internal/base64\r\nunit_golang_1  | github.com/apache/trafficcontrol/vendor/github.com/pkg/errors\r\nunit_golang_1  | golang.org/x/net/ipv4\r\nunit_golang_1  | golang.org/x/net/ipv6\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/vendor/github.com/google/uuid\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/vendor/github.com/lestrrat/go-jwx/buffer\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/vendor/github.com/basho/riak-go-client/rpb/riak\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/vendor/github.com/basho/riak-go-client/rpb/riak_dt\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/vendor/github.com/basho/riak-go-client/rpb/riak_yokozuna\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/vendor/github.com/go-acme/lego/vendor/gopkg.in/square/go-jose.v2\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/vendor/github.com/go-acme/lego/vendor/golang.org/x/net/idna\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/vendor/github.com/lestrrat-go/jwx/jwa\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/vendor/github.com/basho/riak-go-client/rpb/riak_kv\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/vendor/github.com/basho/riak-go-client/rpb/riak_search\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/vendor/github.com/basho/riak-go-client/rpb/riak_ts\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/vendor/github.com/lestrrat/go-jwx/internal/emap\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/vendor/github.com/lestrrat-go/jwx/jwk\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/vendor/github.com/go-acme/lego/vendor/github.com/miekg/dns\r\nunit_golang_1  | github.com/apache/trafficcontrol/vendor/github.com/json-iterator/go\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/vendor/github.com/miekg/dns\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/vendor/github.com/lestrrat/go-jwx/jwk\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/vendor/github.com/basho/riak-go-client\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/vendor/github.com/go-acme/lego/acme/api/internal/secure\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/vendor/github.com/go-acme/lego/acme/api\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/util/ims\r\nunit_golang_1  | github.com/apache/trafficcontrol/lib/go-atscfg\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_monitor/dsdata\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/dbhelpers\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/vendor/github.com/go-acme/lego/certificate\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/v2-client\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/vendor/github.com/go-acme/lego/challenge/http01\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/vendor/github.com/go-acme/lego/challenge/tlsalpn01\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/vendor/github.com/go-acme/lego/registration\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_monitor/config\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/riaksvc\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_monitor/towrap\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/config\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_monitor/todata\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/auth\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_monitor/cache\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/tenant\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/api\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/util/monitorhlp\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/vendor/github.com/go-acme/lego/challenge/dns01\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/vendor/github.com/go-acme/lego/challenge/resolver\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/vendor/github.com/go-acme/lego/lego\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/routing/middleware\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/monitoring\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/asn\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/apitenant\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/parameter\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/coordinate\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/cdnfederation\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/capabilities\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/apicapability\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/cachegroup\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/cachesstats\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/deliveryservice\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/crstats\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/plugin\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/dbdump\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/deliveryservice/consistenthash\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/deliveryservice/request\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/deliveryservice/request/comment\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/deliveryservicerequests\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/deliveryservicesregexes\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/cachegroupparameter\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/division\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/federation_resolvers\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/federations\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/hwinfo\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/invalidationjobs\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/iso\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/login\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/logs\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/origin\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/physlocation\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/ping\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/plugins\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/profileparameter\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/region\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/role\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/server\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/servercapability\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/servercheck\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/servercheck/extensions\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/servicecategory\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/staticdnsentry\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/status\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/steering\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/steeringtargets\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/systeminfo\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/topology\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/trafficstats\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/types\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/profile\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/urisigning\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/user\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/vault\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/deliveryservice/servers\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/crconfig\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/cdn\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/routing\r\nunit_golang_1  | github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang\r\nunit_golang_1  | can't load package: package github.com/apache/trafficcontrol/traffic_ops/traffic_ops_golang/ats/atscdn: \r\nunit_golang_1  | ats/atscdn/headerrewritedotconfig.go:1:1: expected 'package', found 'EOF'\r\ntests_unit_golang_1 exited with code 1\r\n```\r\n</details>\r\n\r\n<!--\r\nLicensed to the Apache Software Foundation (ASF) under one\r\nor more contributor license agreements.See the NOTICE file\r\ndistributed with this work for additional information\r\nregarding copyright ownership.The ASF licenses this file\r\nto you under the Apache License, Version 2.0 (the\r\n\"License\"); you may not use this file except in compliance\r\nwith the License.You may obtain a copy of the License at\r\n\r\nhttp: //www.apache.org/licenses/LICENSE-2.0\r\n\r\nUnless required by applicable law or agreed to in writing,\r\nsoftware distributed under the License is distributed on an\r\n\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\nKIND, either express or implied.See the License for the\r\nspecific language governing permissions and limitations\r\nunder the License.\r\n-->","performed_via_github_app":null,"score":1.0},{"url":"https://api.github.com/repos/inaka/elvis/issues/497","repository_url":"https://api.github.com/repos/inaka/elvis","labels_url":"https://api.github.com/repos/inaka/elvis/issues/497/labels{/name}","comments_url":"https://api.github.com/repos/inaka/elvis/issues/497/comments","events_url":"https://api.github.com/repos/inaka/elvis/issues/497/events","html_url":"https://github.com/inaka/elvis/issues/497","id":431207144,"node_id":"MDU6SXNzdWU0MzEyMDcxNDQ=","number":497,"title":"[Q] Ignore existing issues, without ignoring whole modules.","user":{"login":"define-null","id":879658,"node_id":"MDQ6VXNlcjg3OTY1OA==","avatar_url":"https://avatars1.githubusercontent.com/u/879658?v=4","gravatar_id":"","url":"https://api.github.com/users/define-null","html_url":"https://github.com/define-null","followers_url":"https://api.github.com/users/define-null/followers","following_url":"https://api.github.com/users/define-null/following{/other_user}","gists_url":"https://api.github.com/users/define-null/gists{/gist_id}","starred_url":"https://api.github.com/users/define-null/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/define-null/subscriptions","organizations_url":"https://api.github.com/users/define-null/orgs","repos_url":"https://api.github.com/users/define-null/repos","events_url":"https://api.github.com/users/define-null/events{/privacy}","received_events_url":"https://api.github.com/users/define-null/received_events","type":"User","site_admin":false},"labels":[{"id":109056406,"node_id":"MDU6TGFiZWwxMDkwNTY0MDY=","url":"https://api.github.com/repos/inaka/elvis/labels/enhancement","name":"enhancement","color":"84b6eb","default":true,"description":null}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2019-04-09T21:34:37Z","updated_at":"2020-09-10T12:43:18Z","closed_at":"2020-09-10T12:43:14Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"So there is project with a large code base, that has collected some bad code smells, which we would like to tackle. And with Elvis, as with Dialyzer it's impossible to fix everything at one go, you need to gradually polish off, and clean the code. The problem is, that currently it seems, the only way to ignore certain problems is to add module name to the ignore section of the rule. That could work for relatively small modules, but for large once if could become a real pain.\r\nWhen it comes to dialyzer the common practice it seems - to create a dializer.ignore file with the warnings, that currently exist in the project, and gradually remove one line by one in the eventual phashion.\r\n\r\nBefore introducing any PRs, I'd like to reach out and get some opinion about similar approach for Elvis. Potentially to have a format mode, where elvis would print filename, line, and the error that it encounter. For rules like god modules the line could be equal to 1 or 0 for instance, just to indicate the problem.\r\n\r\nWhat do you think of such an approach? Just to be clear, I want to generate output close to https://github.com/basho/riak/blob/develop/dialyzer.ignore-warnings","performed_via_github_app":null,"score":1.0},{"url":"https://api.github.com/repos/basho/riak_kv/issues/1669","repository_url":"https://api.github.com/repos/basho/riak_kv","labels_url":"https://api.github.com/repos/basho/riak_kv/issues/1669/labels{/name}","comments_url":"https://api.github.com/repos/basho/riak_kv/issues/1669/comments","events_url":"https://api.github.com/repos/basho/riak_kv/issues/1669/events","html_url":"https://github.com/basho/riak_kv/issues/1669","id":315839675,"node_id":"MDU6SXNzdWUzMTU4Mzk2NzU=","number":1669,"title":"Remove erlang_js before releasing riak 2.2.6 ","user":{"login":"bryanhuntesl","id":31992054,"node_id":"MDQ6VXNlcjMxOTkyMDU0","avatar_url":"https://avatars3.githubusercontent.com/u/31992054?v=4","gravatar_id":"","url":"https://api.github.com/users/bryanhuntesl","html_url":"https://github.com/bryanhuntesl","followers_url":"https://api.github.com/users/bryanhuntesl/followers","following_url":"https://api.github.com/users/bryanhuntesl/following{/other_user}","gists_url":"https://api.github.com/users/bryanhuntesl/gists{/gist_id}","starred_url":"https://api.github.com/users/bryanhuntesl/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bryanhuntesl/subscriptions","organizations_url":"https://api.github.com/users/bryanhuntesl/orgs","repos_url":"https://api.github.com/users/bryanhuntesl/repos","events_url":"https://api.github.com/users/bryanhuntesl/events{/privacy}","received_events_url":"https://api.github.com/users/bryanhuntesl/received_events","type":"User","site_admin":false},"labels":[{"id":800482403,"node_id":"MDU6TGFiZWw4MDA0ODI0MDM=","url":"https://api.github.com/repos/basho/riak_kv/labels/3.0","name":"3.0","color":"3586c4","default":false,"description":null}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2018-04-19T11:38:45Z","updated_at":"2020-09-01T08:48:26Z","closed_at":"2020-09-01T08:48:26Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"There is nobody (to the best of my knowledge) using this functionality for several years now, but the dependency persists. \r\n\r\nThe dependency bundles and compiles several of it's dependencies, such as spidermonkey, nspr, and the quizzically named 'system'. These libraries are provided as standard on all contemporary Linux distributions and can be installed easily on OSX using 'brew'.\r\n\r\nAnyway, nobody uses it, and it's currently failing to compile on Freebsd 11 ([gist](https://gist.github.com/bryanhuntesl/461c8d59e03b5b03d21ede3a1f334081)).\r\n\r\nI propose we remove erlang_js from riak_kv, and take a step towards reducing needless complexity. \r\n\r\n","performed_via_github_app":null,"score":1.0},{"url":"https://api.github.com/repos/basho/riak_kv/issues/1771","repository_url":"https://api.github.com/repos/basho/riak_kv","labels_url":"https://api.github.com/repos/basho/riak_kv/issues/1771/labels{/name}","comments_url":"https://api.github.com/repos/basho/riak_kv/issues/1771/comments","events_url":"https://api.github.com/repos/basho/riak_kv/issues/1771/events","html_url":"https://github.com/basho/riak_kv/issues/1771","id":674220909,"node_id":"MDU6SXNzdWU2NzQyMjA5MDk=","number":1771,"title":"Tictac AAE rebuild and unexpected repairs","user":{"login":"martinsumner","id":1628897,"node_id":"MDQ6VXNlcjE2Mjg4OTc=","avatar_url":"https://avatars0.githubusercontent.com/u/1628897?v=4","gravatar_id":"","url":"https://api.github.com/users/martinsumner","html_url":"https://github.com/martinsumner","followers_url":"https://api.github.com/users/martinsumner/followers","following_url":"https://api.github.com/users/martinsumner/following{/other_user}","gists_url":"https://api.github.com/users/martinsumner/gists{/gist_id}","starred_url":"https://api.github.com/users/martinsumner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/martinsumner/subscriptions","organizations_url":"https://api.github.com/users/martinsumner/orgs","repos_url":"https://api.github.com/users/martinsumner/repos","events_url":"https://api.github.com/users/martinsumner/events{/privacy}","received_events_url":"https://api.github.com/users/martinsumner/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2020-08-06T10:52:57Z","updated_at":"2020-08-12T14:21:20Z","closed_at":"2020-08-12T14:21:20Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"When running tictac aae in parallel mode it is necessary to run infrequent but regular rebuilds - whereby the AAE store is rebuilt from the vnode store, and then the AAE tree is rebuilt from the newly built AAE store.\r\n\r\nA strange problem is re-occurring in a stress test.  The stress test is intended to push a cluster by rebuilding all stores at once (which would occur if tictac AAE was enabled in parallel mode for the first time).  The test is run on a 8-node cluster.\r\n\r\nTest process:\r\n\r\n1. Load data (about 1bn objects) split between domainRecordT0 and domainDocumentT0 buckets (keys in different buckets have different types of values, but buckets themselves have same properties), with anti-entropy enable, but tictacaae disabled.\r\n\r\n2. Restart all nodes with tictacaae enabled, and anti-entropy disabled.\r\n\r\n3. After restart start fresh load of data split between buckets domainRecordT1 and domainDocumentT1.\r\n\r\n4. Confirm all stores build, all trees build and there are no errors.\r\n\r\n5. Restart all nodes again, after deleting the tictac stores.\r\n\r\n6. After restart start fresh load of data split between buckets domainRecordT2 and domainDocumentT2.\r\n\r\n7. Confirm all stores build, all trees build and there are no errors.\r\n\r\nAt this stage there is a strange issue.  The stores and trees rebuild, but once a pair of partitions in a preflist both have trees rebuilt, we begin to see key_deltas being discovered - where one vnode has a bunch of keys, and the other doesn't.\r\n\r\nThe intriguing thing, is that almost all (>> 99%) the keys which have deltas are in bucket domainRecordT1 - there ar eno deltas discovered in domainDocumentT1 (or any other buckets).\r\n\r\n8. Restart all nodes again, after deleting the tictac stores.  This time don't do a data load.\r\n\r\n9. Confirm all stores build, all trees build and there are no errors.\r\n\r\nInterestingly we don't see the key_deltas now.\r\n\r\n10. Restart all nodes again, after deleting the tictac stores.\r\n\r\n11. After restart start fresh load of data split between buckets domainRecordT3 and domainDocumentT3.\r\n\r\n12. Confirm all stores build, all trees build and there are no errors.\r\n\r\nAgain the strange issue re-occurs.  This time almost all the key deltas are in domainRecordT2, with a small amount (<1%) in domainRecordT1.\r\n\r\nSome observations:\r\n\r\nA. The key deltas are all keys loaded during the previous rebuild period (observing the timestamp in the vector clock).\r\n\r\nB. There are read repairs occurring in the cluster, but most key deltas don't result in read repairs (the count of read repairs is much less than the total delta key count).  So most of the deltas discovered do not reflect a genuine delta between the vnode stores, but are a delta between the newly built key stores.\r\n\r\nC. The loading of the data during the test run, starts before the rebuild is triggered.  So when the test is started the domainRecordT{N} bucket with which we will have the problem is alphabetically the highest bucket in the store, but by the time the rebuild is triggered there will already be some domainRecordT{N+1} entries.\r\n\r\nD. The \"deltas\" discovered represent a minority of the objects loaded into that bucket (<1%) but still a large number (>100K).\r\n\r\nThis is just so weird.  I've not yet managed to recreate in a smaller setup, and each cycle (e.g. step 10-12) takes about 24 hours, so experimenting to try and tie this down is hard to do.\r\n\r\nCurrently this is occurring in the planned 2.9.6 release.  I've not bene able to confirm if it occurs in current releases - I don't know if it is introduced by 2.9.6 changes.\r\n\r\n\r\n\r\n","performed_via_github_app":null,"score":1.0},{"url":"https://api.github.com/repos/basho/riak_repl/issues/799","repository_url":"https://api.github.com/repos/basho/riak_repl","labels_url":"https://api.github.com/repos/basho/riak_repl/issues/799/labels{/name}","comments_url":"https://api.github.com/repos/basho/riak_repl/issues/799/comments","events_url":"https://api.github.com/repos/basho/riak_repl/issues/799/events","html_url":"https://github.com/basho/riak_repl/issues/799","id":503389669,"node_id":"MDU6SXNzdWU1MDMzODk2Njk=","number":799,"title":"Failure to complete full-sync on hitting soft retry limit","user":{"login":"martinsumner","id":1628897,"node_id":"MDQ6VXNlcjE2Mjg4OTc=","avatar_url":"https://avatars0.githubusercontent.com/u/1628897?v=4","gravatar_id":"","url":"https://api.github.com/users/martinsumner","html_url":"https://github.com/martinsumner","followers_url":"https://api.github.com/users/martinsumner/followers","following_url":"https://api.github.com/users/martinsumner/following{/other_user}","gists_url":"https://api.github.com/users/martinsumner/gists{/gist_id}","starred_url":"https://api.github.com/users/martinsumner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/martinsumner/subscriptions","organizations_url":"https://api.github.com/users/martinsumner/orgs","repos_url":"https://api.github.com/users/martinsumner/repos","events_url":"https://api.github.com/users/martinsumner/events{/privacy}","received_events_url":"https://api.github.com/users/martinsumner/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2019-10-07T11:23:27Z","updated_at":"2020-05-12T16:22:15Z","closed_at":"2020-05-12T16:22:15Z","author_association":"NONE","active_lock_reason":null,"body":"Potentially related to - https://github.com/basho/riak_repl/issues/772\r\n\r\nThe test https://github.com/basho/riak_test/blob/develop-2.9/tests/repl_aae_fullsync_blocked.erl fails intermittently.\r\n\r\nIt fails when it uses an intercept to stop a full-sync from working on some vnodes, and checks the right number of vnode sync failures has occurred on completion.\r\n\r\nWhen the test fails, it fails as full-sync is never considered complete.  The difference between success and failure is related to the ordering of the vnodes which the full-sync tries.  If the last vnode to be sync'd does sync OK (as it is not one with an intercepted function), then the test passes, and the correct number of vnodes failures are reported.  If the last vnode to be sync'd is one of those to not sync though, although the same work has completed/failed - the full-sync is never recorded as complete.\r\n\r\nThe cause of this appears to be that on hitting the soft retry limit https://github.com/basho/riak_repl/blob/24c6e8f408450cdf48b7259f9f0ef0778f94180d/src/riak_repl2_fscoordinator.erl#L561-L571 the function `maybe_complete_fullsync/2` isn't called.  Unlike on a hard failure - https://github.com/basho/riak_repl/blob/24c6e8f408450cdf48b7259f9f0ef0778f94180d/src/riak_repl2_fscoordinator.erl#L590 - and unlike on a success - https://github.com/basho/riak_repl/blob/24c6e8f408450cdf48b7259f9f0ef0778f94180d/src/riak_repl2_fscoordinator.erl#L447.\r\n\r\n\r\n","performed_via_github_app":null,"score":1.0},{"url":"https://api.github.com/repos/basho/riak_kv/issues/1754","repository_url":"https://api.github.com/repos/basho/riak_kv","labels_url":"https://api.github.com/repos/basho/riak_kv/issues/1754/labels{/name}","comments_url":"https://api.github.com/repos/basho/riak_kv/issues/1754/comments","events_url":"https://api.github.com/repos/basho/riak_kv/issues/1754/events","html_url":"https://github.com/basho/riak_kv/issues/1754","id":588464406,"node_id":"MDU6SXNzdWU1ODg0NjQ0MDY=","number":1754,"title":"Recent writes loss in the presence of heavy handoff activity","user":{"login":"keynslug","id":1148366,"node_id":"MDQ6VXNlcjExNDgzNjY=","avatar_url":"https://avatars0.githubusercontent.com/u/1148366?v=4","gravatar_id":"","url":"https://api.github.com/users/keynslug","html_url":"https://github.com/keynslug","followers_url":"https://api.github.com/users/keynslug/followers","following_url":"https://api.github.com/users/keynslug/following{/other_user}","gists_url":"https://api.github.com/users/keynslug/gists{/gist_id}","starred_url":"https://api.github.com/users/keynslug/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/keynslug/subscriptions","organizations_url":"https://api.github.com/users/keynslug/orgs","repos_url":"https://api.github.com/users/keynslug/repos","events_url":"https://api.github.com/users/keynslug/events{/privacy}","received_events_url":"https://api.github.com/users/keynslug/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":13,"created_at":"2020-03-26T14:29:25Z","updated_at":"2020-09-01T08:45:05Z","closed_at":"2020-09-01T08:45:04Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Riak KV may lost recent writes in the presence of heavy handoff activity.\r\n\r\nI've [made a test][1] with the help of Jepsen framework specifically to investigate this kind of buggy behaviour because it hit us several times already in our production environment. This test observes an occurence of this bug almost every run to be honest. An ordinary test run follows roughly these steps:\r\n1. Set up 5 riak kv nodes in a docker-compose environment.\r\n2. Join them together in a single cluster.\r\n3. Start pouring reads and writes over some small keyspace (10 keys was sufficient most of the time) concurrently through some small number of clients (again, 40 was sufficient), each putting as little as 1 rps. Note that this process would continue until the very end. Note also that clients take effort to ensure that writes are _causally consistent_ (using past reads' vclocks) and performed with n = 3, r/w/pr/pw = quorum, lww = false, allow_mult = false.\r\n4. Give the cluster 40 seconds to stabilise. Most of the time the cluster would still have a number of ongoing transfers by this time, but that's ok.\r\n5. Then, set up 2 extra nodes and join them in the cluster.\r\n6. Again, give the cluster another 120 seconds to stabilise, after that order those 2 extra nodes to _leave_ the cluster.\r\n7. Wait for them to leave cleanly. This is the most time consuming phase, but usually 200 seconds was sufficient for it to finish.\r\n8. Repeat the process starting from step 4, until the test run time limit passes. Usually I ran it for 15 minutes, it was enough for 3 such cycles.\r\n\r\nI have attached a [Jepsen report archive][2] of one such test run. Most notable observations are in `results.edn`, here's an excerpt, redacted for brewity:\r\n```\r\n...\r\n\"jepsen638479067-6\" {:valid? false,\r\n    :configs (...),\r\n    :final-paths ([{:op {:process 25,\r\n                        :type :ok,\r\n                        :f :read,\r\n                        :value 20,\r\n                        :index 16396,\r\n                        :time 207095405146,\r\n                        :vclock \"a85hYGD...JZAA==\"},\r\n                    :model #knossos.model.CASRegister{:value 20}}\r\n                    {:op {:process 25,\r\n                        :type :ok,\r\n                        :f :read,\r\n                        :value 31,\r\n                        :index 16408,\r\n                        :time 207247072654,\r\n                        :vclock \"a85hYGD...SyAA==\"},\r\n                    :model #knossos.model.Inconsistent{:msg \"can't read 31 from register 20\"}}]),\r\n    :previous-ok {...},\r\n    :last-op {...},\r\n    :op {...}},\r\n...\r\n```\r\n\r\nBasically this all means that the client 25 while operating on key `jepsen638479067-6` read value `20` _and subsequently_ read value `31` quite unexpectedly, all **in the absence of concurrent writes** under this key.\r\n\r\nSkimming through `history.edn`, here's relevant history fragment, again redacted a bit for brewity:\r\n```\r\n{:type :invoke,  :f :write,  :value 31,   :process 27,  :time 205105199455,        :index 16232}\r\n{:type :ok,      :f :write,  :value 31,   :process 27,  :time 205110025753,  ...,  :index 16234}\r\n{:type :invoke,  :f :write,  :value 5,    :process 26,  :time 205222111586,        :index 16245}\r\n{:type :ok,      :f :write,  :value 5,    :process 26,  :time 205224722230,  ...,  :index 16246}\r\n{:type :invoke,  :f :read,   :value nil,  :process 25,  :time 205256891475,        :index 16250}\r\n{:type :ok,      :f :read,   :value 5,    :process 25,  :time 205260160969,  ...,  :index 16252}\r\n{:type :invoke,  :f :read,   :value nil,  :process 24,  :time 206523664708,        :index 16343}\r\n{:type :ok,      :f :read,   :value 5,    :process 24,  :time 206526735511,  ...,  :index 16344}\r\n{:type :invoke,  :f :write,  :value 30,   :process 26,  :time 206664972077,        :index 16356}\r\n{:type :ok,      :f :write,  :value 30,   :process 26,  :time 206673674701,  ...,  :index 16358}\r\n{:type :invoke,  :f :write,  :value 20,   :process 27,  :time 206712230895,        :index 16361}\r\n{:type :ok,      :f :write,  :value 20,   :process 27,  :time 206715122992,  ...,  :index 16362}\r\n{:type :invoke,  :f :read,   :value nil,  :process 25,  :time 207091635937,        :index 16394}\r\n{:type :ok,      :f :read,   :value 20,   :process 25,  :time 207095405146,  ...,  :index 16396}\r\n{:type :invoke,  :f :read,   :value nil,  :process 25,  :time 207244686086,        :index 16407}\r\n{:type :ok,      :f :read,   :value 31,   :process 25,  :time 207247072654,  ...,  :index 16408}\r\n```\r\n\r\nIt can be seen clearly that an act of writing `31` at index _16232_ effected read at index _16407_, though in the meantime 3 subsequent writes have succeeded (_16245_, _16356_, _16361_).\r\n\r\n[1]: https://github.com/rbkmoney/riak-jepsenized/tree/ft/cluster-scaler\r\n[2]: https://github.com/basho/riak_kv/files/4387774/20200325T191747.000Z.tar.gz\r\n\r\n## Affected version(s)\r\n\r\n```\r\nriak_kv_version       : riak_kv-2.9.1.1\r\nriak_core_version     : riak_kv-2.9.1\r\nriak_api_version      : riak_kv-2.9.1\r\nriak_pb_version       : riak_kv-2.9.1\r\nriak_pipe_version     : riak_kv-2.9.1\r\n```\r\n\r\nThough it won't hurt to mention that I have successfully reproduced this bug under riak 2.2.3 as well.\r\n\r\n## Steps to reproduce\r\n\r\n1. Clone [aforementioned repo][1].\r\n\r\n2. Start a test run with:\r\n\r\n    ```\r\n    $ cd system\r\n    $ docker-compose up -d\r\n    $ docker-compose exec control ./run-test\r\n    ```\r\n\r\n3. Wait for it to finish cleanly.\r\n\r\n    Note that tearing down node with `riak stop` on 2.9.1 may become stuck for some reason at the end of a run, simple ^C would be fine then.\r\n\r\n4. Look at the analysis report in `store/latest/results.edn`.\r\n\r\n## Suspected cause\r\n\r\nEssentially this is caused by the fact that put fsm does not account for the possibility of _multiple_ forwards to another coordinator. I'll try to illustrate:\r\n1. Node _n1_ receives put request and spin up a fsm to handle it.\r\n2. This fsm @ _n1_ [decides][3] to forward request to another node _n2_ (which happens to be in the preflist from the point of view of _n1_), spawns fsm there and [waits for it to acknowledge][4] their readiness, given [`retry_put_coordinator_failure` is `true`][5] which is the [default][6] per cuttlefish schema.\r\n3. Again, this fsm @ _n2_ **may** as I suspect under certain circumstances decide that it is in fact not in the preflist anymore, and forward request to another node _n3_, again ending up waiting for acknowlege afterwards. Here, possible circumstances are high vnode churn in the cluster.\r\n4. The fsm @ _n3_ happily acknowlegdes readiness to fsm @ _n2_ and proceeds with coordinating put operation.\r\n5. The fsm @ _n2_ receives acknowledgement and stops, without any other interaction with fsm @ _n1_.\r\n6. This fsm @ _n1_ times out waiting for the acknowledgement and [retries forwarding request][7] after 3 seconds by default, whereas original request is quite likely handled successfully already at this point.\r\n\r\nThis is why I believe there are occasional _unrecognized messages_ in logs, which are extraneous responses to original requests retried unwarrantedly:\r\n```\r\n2020-03-25T19:19:00.386369Z [error] Unrecognized message {15919439,{ok,{r_object,<<\"registers\">>,<<\"jepsen638479067-8\">>,[{r_content,{dict,3,16,16,8,80,48,{[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]},{{[],[],[],[],[],[],[],[],[],[],[[<<\"X-Riak-VTag\">>,50,113,86,90,100,101,73,71,104,90,49,76,106,117,113,77,121,106,86,52,98,70]],[[<<\"index\">>]],[],[[<<\"X-Riak-Last-Modified\">>|{1585,163940,383362}]],[],[]}}},<<\"37\">>}],[{<<\"...\">>,{13,63752383125}},{<<188,169,128,137,39,255,43,155>>,{5,63752383126}},{<<188,169,128,137,39,255,44,215,0,0,0,1>>,{28,63752383133}},{<<91,17,173,142,39,242,0,107>>,{1,63752383139}},{<<187,196,68,144,40,0,3,163>>,{8,63752383140}},{<<188,169,128,137,39,255,44,166>>,{7,63752383140}}],{dict,1,16,16,8,80,48,{[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]},{{[],[],[],[],[],[],[],[],[],[],[],[],[],[],[[clean|true]],[]}}},undefined}}}\r\n2020-03-25T19:19:02.354180Z [error] Unrecognized message {53908694,{ok,{r_object,<<\"registers\">>,<<\"jepsen638479067-8\">>,[{r_content,{dict,3,16,16,8,80,48,{[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]},{{[],[],[],[],[],[],[],[],[],[],[[<<\"X-Riak-VTag\">>,49,85,73,73,113,78,109,51,122,50,116,66,102,119,90,103,55,81,100,57,76,49]],[[<<\"index\">>]],[],[[<<\"X-Riak-Last-Modified\">>|{1585,163942,349490}]],[],[]}}},<<\"17\">>}],[{<<\"...\">>,{13,63752383125}},{<<188,169,128,137,39,255,43,155>>,{5,63752383126}},{<<188,169,128,137,39,255,44,215,0,0,0,1>>,{28,63752383133}},{<<91,17,173,142,39,242,0,107>>,{1,63752383139}},{<<188,169,128,137,39,255,44,166>>,{7,63752383140}},{<<187,196,68,144,40,0,3,163>>,{11,63752383142}}],{dict,1,16,16,8,80,48,{[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]},{{[],[],[],[],[],[],[],[],[],[],[],[],[],[],[[clean|true]],[]}}},undefined}}}\r\n```\r\n\r\nOne more evidence which strengthen my suspicion is that turning off `retry_put_coordinator_failure` makes this buggy behaviuor go away with quite high probability, if not completely, given the number of test runs I've performed so far.\r\n\r\n[3]: https://github.com/basho/riak_kv/blob/f6df1750c5e72874fa10a298e04f00e5b44d736d/src/riak_kv_put_fsm.erl#L1060\r\n[4]: https://github.com/basho/riak_kv/blob/f6df1750c5e72874fa10a298e04f00e5b44d736d/src/riak_kv_put_fsm.erl#L214\r\n[5]: https://github.com/basho/riak_kv/blob/f6df1750c5e72874fa10a298e04f00e5b44d736d/src/riak_kv_put_fsm.erl#L189\r\n[6]: https://github.com/basho/riak_kv/blob/03d37d0823d9333e4360734ca1144ba1f38fe428/priv/riak_kv.schema#L437\r\n[7]: https://github.com/basho/riak_kv/blob/f6df1750c5e72874fa10a298e04f00e5b44d736d/src/riak_kv_put_fsm.erl#L221\r\n\r\n## Additional observations\r\n\r\nMoreover it's not clear for me why acknowledgement is being sent [in the _execute_ state][8] of an fsm, but not in the _validate_ state since there's a possibility that _execute_ will be skipped altogether, in the event of [parameter violation][9] for example.\r\n\r\n[8]: https://github.com/basho/riak_kv/blob/f6df1750c5e72874fa10a298e04f00e5b44d736d/src/riak_kv_put_fsm.erl#L444\r\n[9]: https://github.com/basho/riak_kv/blob/f6df1750c5e72874fa10a298e04f00e5b44d736d/src/riak_kv_put_fsm.erl#L338\r\n","performed_via_github_app":null,"score":1.0},{"url":"https://api.github.com/repos/vernemq/vernemq/issues/740","repository_url":"https://api.github.com/repos/vernemq/vernemq","labels_url":"https://api.github.com/repos/vernemq/vernemq/issues/740/labels{/name}","comments_url":"https://api.github.com/repos/vernemq/vernemq/issues/740/comments","events_url":"https://api.github.com/repos/vernemq/vernemq/issues/740/events","html_url":"https://github.com/vernemq/vernemq/issues/740","id":333984677,"node_id":"MDU6SXNzdWUzMzM5ODQ2Nzc=","number":740,"title":"URL parameters for webhooks","user":{"login":"baranga","id":467902,"node_id":"MDQ6VXNlcjQ2NzkwMg==","avatar_url":"https://avatars1.githubusercontent.com/u/467902?v=4","gravatar_id":"","url":"https://api.github.com/users/baranga","html_url":"https://github.com/baranga","followers_url":"https://api.github.com/users/baranga/followers","following_url":"https://api.github.com/users/baranga/following{/other_user}","gists_url":"https://api.github.com/users/baranga/gists{/gist_id}","starred_url":"https://api.github.com/users/baranga/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/baranga/subscriptions","organizations_url":"https://api.github.com/users/baranga/orgs","repos_url":"https://api.github.com/users/baranga/repos","events_url":"https://api.github.com/users/baranga/events{/privacy}","received_events_url":"https://api.github.com/users/baranga/received_events","type":"User","site_admin":false},"labels":[{"id":216744077,"node_id":"MDU6TGFiZWwyMTY3NDQwNzc=","url":"https://api.github.com/repos/vernemq/vernemq/labels/enhancement","name":"enhancement","color":"84b6eb","default":true,"description":null}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":11,"created_at":"2018-06-20T08:59:02Z","updated_at":"2020-06-17T16:49:23Z","closed_at":"2020-06-17T16:49:23Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Environment\r\n\r\n- VerneMQ Version: 1.3.1\r\n- OS: Debian\r\n\r\n### Expected behavior\r\n\r\nOne can specify URL parameters when registering webhooks:\r\n```\r\n$ vmq-admin webhooks register hook=auth_on_register endpoint=my-service/hook?key=value\r\nDone\r\n```\r\n\r\n### Actual behaviour\r\n\r\nRegister fails with of pretty opaque error message:\r\n```\r\n$ vmq-admin webhooks register hook=auth_on_register endpoint=my-service/hook?key=value\r\nToo many equal signs in argument: \"endpoint=my-service/hook?key=value\"\r\n```\r\n\r\nI've looked into this and found that this behavior is a issue with `clique`. I already filled a issue (basho/clique#85) and pr (basho/clique#86) over there so this is more of a reminder to update the clique dependency as soon as they fix it.","performed_via_github_app":null,"score":1.0},{"url":"https://api.github.com/repos/basho/riak/issues/1002","repository_url":"https://api.github.com/repos/basho/riak","labels_url":"https://api.github.com/repos/basho/riak/issues/1002/labels{/name}","comments_url":"https://api.github.com/repos/basho/riak/issues/1002/comments","events_url":"https://api.github.com/repos/basho/riak/issues/1002/events","html_url":"https://github.com/basho/riak/issues/1002","id":550801928,"node_id":"MDU6SXNzdWU1NTA4MDE5Mjg=","number":1002,"title":"bucket-type create broken in 3.0","user":{"login":"martincox","id":3169010,"node_id":"MDQ6VXNlcjMxNjkwMTA=","avatar_url":"https://avatars0.githubusercontent.com/u/3169010?v=4","gravatar_id":"","url":"https://api.github.com/users/martincox","html_url":"https://github.com/martincox","followers_url":"https://api.github.com/users/martincox/followers","following_url":"https://api.github.com/users/martincox/following{/other_user}","gists_url":"https://api.github.com/users/martincox/gists{/gist_id}","starred_url":"https://api.github.com/users/martincox/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/martincox/subscriptions","organizations_url":"https://api.github.com/users/martincox/orgs","repos_url":"https://api.github.com/users/martincox/repos","events_url":"https://api.github.com/users/martincox/events{/privacy}","received_events_url":"https://api.github.com/users/martincox/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2020-01-16T13:18:30Z","updated_at":"2020-04-07T08:38:26Z","closed_at":"2020-04-07T08:38:26Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Does not work when trying to create a new bucket.\r\n\r\nAt the same time, I also wonder if it's worthwhile considering moving away from the unfortunate need to create a json formatted props string when specifying config at the point of creation. I'm sure that this has been talked about before, there may even be open issues for it somewhere.\r\n\r\nRPC to 'riak@127.0.0.1' failed: {'EXIT', {function_clause, [{riak_kv_console,bucket_type_create, [[\"foo\"]], [{file, \"/data/jenkins-agent/workspace/riak-3.1/rel/pkg/out/BUILD /riak-3.1-2-gd8acf0a/_build/default/lib/riak_kv/src/riak_kv_console.erl\"}, {line,499}]}, {rpc,'-handle_call_call/6-fun-0-',5, [{file,\"rpc.erl\"},{line,197}]}]}} ","performed_via_github_app":null,"score":1.0},{"url":"https://api.github.com/repos/basho/basho_docs/issues/2421","repository_url":"https://api.github.com/repos/basho/basho_docs","labels_url":"https://api.github.com/repos/basho/basho_docs/issues/2421/labels{/name}","comments_url":"https://api.github.com/repos/basho/basho_docs/issues/2421/comments","events_url":"https://api.github.com/repos/basho/basho_docs/issues/2421/events","html_url":"https://github.com/basho/basho_docs/issues/2421","id":205215728,"node_id":"MDU6SXNzdWUyMDUyMTU3Mjg=","number":2421,"title":"Java Client - Certificate Based Authentication [JIRA: DOC-842]","user":{"login":"shaneutt","id":5332524,"node_id":"MDQ6VXNlcjUzMzI1MjQ=","avatar_url":"https://avatars2.githubusercontent.com/u/5332524?v=4","gravatar_id":"","url":"https://api.github.com/users/shaneutt","html_url":"https://github.com/shaneutt","followers_url":"https://api.github.com/users/shaneutt/followers","following_url":"https://api.github.com/users/shaneutt/following{/other_user}","gists_url":"https://api.github.com/users/shaneutt/gists{/gist_id}","starred_url":"https://api.github.com/users/shaneutt/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/shaneutt/subscriptions","organizations_url":"https://api.github.com/users/shaneutt/orgs","repos_url":"https://api.github.com/users/shaneutt/repos","events_url":"https://api.github.com/users/shaneutt/events{/privacy}","received_events_url":"https://api.github.com/users/shaneutt/received_events","type":"User","site_admin":false},"labels":[{"id":184839144,"node_id":"MDU6TGFiZWwxODQ4MzkxNDQ=","url":"https://api.github.com/repos/basho/basho_docs/labels/JIRA:%20To%20Do","name":"JIRA: To Do","color":"ededed","default":false,"description":null}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2017-02-03T17:11:47Z","updated_at":"2020-02-07T15:38:13Z","closed_at":"2020-02-07T15:38:13Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"The current documentation says that Certificate Based Authentication for the Java client is unsupported.\r\n\r\nExample:\r\n\r\nhttps://docs.basho.com/riak/kv/2.2.0/developing/usage/security/java/#certificate-based-authentication\r\n\r\nThe client does now support this.\r\n\r\n# Java Key Store (JKS) Examples:\r\n\r\nhttps://github.com/basho/riak-java-client/blob/9bed4fb9502b410665160b06f7382d02988dbf3e/src/test/java/com/basho/riak/client/core/operations/itest/ITestJKSSecuredConnection.java#L16-L15\r\nhttps://github.com/basho/riak-java-client/blob/riak-client-2.1.1/src/test/java/com/basho/riak/client/core/operations/itest/ITestJKSSecuredConnection.java#L50\r\nhttps://github.com/basho/riak-java-client/blob/riak-client-2.1.1/src/test/java/com/basho/riak/client/core/operations/itest/ITestJKSSecuredConnection.java\r\n\r\n# PEM Examples:\r\n\r\nhttps://github.com/basho/riak-java-client/blob/9bed4fb9502b410665160b06f7382d02988dbf3e/src/test/java/com/basho/riak/client/core/operations/itest/ITestPemSecuredConnection.java#L15-L15\r\nhttps://github.com/basho/riak-java-client/blob/riak-client-2.1.1/src/test/java/com/basho/riak/client/core/operations/itest/ITestPemSecuredConnection.java#L46-L50\r\nhttps://github.com/basho/riak-java-client/blob/riak-client-2.1.1/src/test/java/com/basho/riak/client/core/operations/itest/RiakPemConnection.java\r\n\r\n# Getting the certs imported correctly:\r\n\r\nhttps://github.com/basho/riak-java-client/blob/develop/Makefile#L43-L62","performed_via_github_app":null,"score":1.0},{"url":"https://api.github.com/repos/basho/riak_kv/issues/1676","repository_url":"https://api.github.com/repos/basho/riak_kv","labels_url":"https://api.github.com/repos/basho/riak_kv/issues/1676/labels{/name}","comments_url":"https://api.github.com/repos/basho/riak_kv/issues/1676/comments","events_url":"https://api.github.com/repos/basho/riak_kv/issues/1676/events","html_url":"https://github.com/basho/riak_kv/issues/1676","id":402627826,"node_id":"MDU6SXNzdWU0MDI2Mjc4MjY=","number":1676,"title":"The aae-fold needs a PB API","user":{"login":"martinsumner","id":1628897,"node_id":"MDQ6VXNlcjE2Mjg4OTc=","avatar_url":"https://avatars0.githubusercontent.com/u/1628897?v=4","gravatar_id":"","url":"https://api.github.com/users/martinsumner","html_url":"https://github.com/martinsumner","followers_url":"https://api.github.com/users/martinsumner/followers","following_url":"https://api.github.com/users/martinsumner/following{/other_user}","gists_url":"https://api.github.com/users/martinsumner/gists{/gist_id}","starred_url":"https://api.github.com/users/martinsumner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/martinsumner/subscriptions","organizations_url":"https://api.github.com/users/martinsumner/orgs","repos_url":"https://api.github.com/users/martinsumner/repos","events_url":"https://api.github.com/users/martinsumner/events{/privacy}","received_events_url":"https://api.github.com/users/martinsumner/received_events","type":"User","site_admin":false},"labels":[{"id":1290152206,"node_id":"MDU6TGFiZWwxMjkwMTUyMjA2","url":"https://api.github.com/repos/basho/riak_kv/labels/2.9.1","name":"2.9.1","color":"bfdadc","default":false,"description":"Extension to release 2.9"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2019-01-24T10:00:07Z","updated_at":"2020-02-25T10:21:54Z","closed_at":"2020-02-25T10:21:54Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"See https://github.com/basho/riak_kv/pull/1675","performed_via_github_app":null,"score":1.0},{"url":"https://api.github.com/repos/basho/riak_test/issues/1333","repository_url":"https://api.github.com/repos/basho/riak_test","labels_url":"https://api.github.com/repos/basho/riak_test/issues/1333/labels{/name}","comments_url":"https://api.github.com/repos/basho/riak_test/issues/1333/comments","events_url":"https://api.github.com/repos/basho/riak_test/issues/1333/events","html_url":"https://github.com/basho/riak_test/issues/1333","id":516066543,"node_id":"MDU6SXNzdWU1MTYwNjY1NDM=","number":1333,"title":"Intercepts in OTP20","user":{"login":"martinsumner","id":1628897,"node_id":"MDQ6VXNlcjE2Mjg4OTc=","avatar_url":"https://avatars0.githubusercontent.com/u/1628897?v=4","gravatar_id":"","url":"https://api.github.com/users/martinsumner","html_url":"https://github.com/martinsumner","followers_url":"https://api.github.com/users/martinsumner/followers","following_url":"https://api.github.com/users/martinsumner/following{/other_user}","gists_url":"https://api.github.com/users/martinsumner/gists{/gist_id}","starred_url":"https://api.github.com/users/martinsumner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/martinsumner/subscriptions","organizations_url":"https://api.github.com/users/martinsumner/orgs","repos_url":"https://api.github.com/users/martinsumner/repos","events_url":"https://api.github.com/users/martinsumner/events{/privacy}","received_events_url":"https://api.github.com/users/martinsumner/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2019-11-01T11:49:27Z","updated_at":"2020-04-20T08:02:46Z","closed_at":"2020-04-20T08:02:46Z","author_association":"NONE","active_lock_reason":null,"body":"The `verify_corruption_filtering` test fails consistently in bitcask on `develop-3.0`.\r\n\r\nWhen running the test bitcask constantly crashes during the leave:\r\n\r\n```\r\nexception error: {{badfun,#Fun<riak_kv_bitcask_backend.1.70625586>},[{bitcask,'-fold/6-fun-0-',11,[{file,\"/Users/martinsumner/dbroot/basho/riak/_build/default/lib/bitcask/src/bitcask.erl\"},{line,411}]},{bitcask_fileops,fold_int_loop,5,[{file,\"/Users/martinsumner/dbroot/basho/riak/_build/default/lib/bitcask/src/bitcask_fileops.erl\"},{line,558}]},{bitcask_fileops,fold_file_loop,8,[{file,\"/Users/martinsumner/dbroot/basho/riak/_build/default/lib/bitcask/src/bitcask_fileops.erl\"},{line,716}]},{bitcask_fileops,fold,3,[{file,\"/Users/martinsumner/dbroot/basho/riak/_build/default/lib/bitcask/src/bitcask_fileops.erl\"},{line,390}]},{bitcask,subfold,3,[{file,\"/Users/martinsumner/dbroot/basho/riak/_build/default/lib/bitcask/src/bitcask.erl\"},{line,522}]},{bitcask_nifs,keydir_frozen,4,[{file,\"/Users/martinsumner/dbroot/basho/riak/_build/default/lib/bitcask/src/bitcask_nifs.erl\"},{line,272}]},{riak_kv_bitcask_backend_orig,'-fold_objects_orig/4-fun-1-',5,[{file,\"/Users/martinsumner/dbroot/basho/riak/_build/default/lib/riak_kv/src/riak_kv_bitcask_backend.erl\"},{line,355}]},{riak_kv_worker,handle_work,3,[{file,\"/Users/martinsumner/dbroot/basho/riak/_build/default/lib/riak_kv/src/riak_kv_worker.erl\"},{line,46}]}]}\r\n```\r\n\r\nThis bad function is a function passed in from the `riak_kv_bitcask_backend` module on startup.  This module (although not the function) though is dynamically changed during the test (following startup) to add intercept functions.\r\n\r\nIf I move the key_transform function into a separate, dedicated module - the test will pass.\r\n\r\nIt looks like adding intercepts causes problems to functions in other repos that referenced back to the edited module (even if the function itself is not changed). ","performed_via_github_app":null,"score":1.0},{"url":"https://api.github.com/repos/martinsumner/leveled/issues/311","repository_url":"https://api.github.com/repos/martinsumner/leveled","labels_url":"https://api.github.com/repos/martinsumner/leveled/issues/311/labels{/name}","comments_url":"https://api.github.com/repos/martinsumner/leveled/issues/311/comments","events_url":"https://api.github.com/repos/martinsumner/leveled/issues/311/events","html_url":"https://github.com/martinsumner/leveled/issues/311","id":587836830,"node_id":"MDU6SXNzdWU1ODc4MzY4MzA=","number":311,"title":"Slow queries when index entries deleted","user":{"login":"martinsumner","id":1628897,"node_id":"MDQ6VXNlcjE2Mjg4OTc=","avatar_url":"https://avatars0.githubusercontent.com/u/1628897?v=4","gravatar_id":"","url":"https://api.github.com/users/martinsumner","html_url":"https://github.com/martinsumner","followers_url":"https://api.github.com/users/martinsumner/followers","following_url":"https://api.github.com/users/martinsumner/following{/other_user}","gists_url":"https://api.github.com/users/martinsumner/gists{/gist_id}","starred_url":"https://api.github.com/users/martinsumner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/martinsumner/subscriptions","organizations_url":"https://api.github.com/users/martinsumner/orgs","repos_url":"https://api.github.com/users/martinsumner/repos","events_url":"https://api.github.com/users/martinsumner/events{/privacy}","received_events_url":"https://api.github.com/users/martinsumner/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2020-03-25T16:45:26Z","updated_at":"2020-04-07T17:00:23Z","closed_at":"2020-04-07T17:00:23Z","author_association":"OWNER","active_lock_reason":null,"body":"If there is a large leveled store, then we add a lot of index entries, then delete those index entries - then the time taken to conclude an index query will be proportional to the number of removed entries not the active entries.\r\n\r\nThis is because leveled tombstones in the ledger are maintained as the ledger is compacted until the tombstone reaches the basement.  When it reaches the basement, the tombstone will be reaped - as it is now clear that there are no further objects to be erased.\r\n\r\nSo if we add a {Key1, V1}\r\nthen alter by adding {Key1, V2}\r\nthen delete with {Key1, tomb}\r\n\r\nThe tomb must be maintained in the store even after it has merged through {Key1, V2} so as to not resurrect {Key1, V1}.  Only when a tomb is merged into the basement level do we know it has no more victims to kill and so the tomb can be erased i.e. the tomb will not be written to the basement level.\r\n\r\nThere may be certain workloads that result in very infrequent merges to the basement, and hence there may be large files full of tombstones that are repeatedly scanned for queries to discover all the tombstones (which don't result in any results).  So the result may take a long time to return even when there are very few actual results.","performed_via_github_app":null,"score":1.0},{"url":"https://api.github.com/repos/basho/riak/issues/1004","repository_url":"https://api.github.com/repos/basho/riak","labels_url":"https://api.github.com/repos/basho/riak/issues/1004/labels{/name}","comments_url":"https://api.github.com/repos/basho/riak/issues/1004/comments","events_url":"https://api.github.com/repos/basho/riak/issues/1004/events","html_url":"https://github.com/basho/riak/issues/1004","id":563289611,"node_id":"MDU6SXNzdWU1NjMyODk2MTE=","number":1004,"title":"Format error hash tree","user":{"login":"sAws","id":10175884,"node_id":"MDQ6VXNlcjEwMTc1ODg0","avatar_url":"https://avatars2.githubusercontent.com/u/10175884?v=4","gravatar_id":"","url":"https://api.github.com/users/sAws","html_url":"https://github.com/sAws","followers_url":"https://api.github.com/users/sAws/followers","following_url":"https://api.github.com/users/sAws/following{/other_user}","gists_url":"https://api.github.com/users/sAws/gists{/gist_id}","starred_url":"https://api.github.com/users/sAws/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/sAws/subscriptions","organizations_url":"https://api.github.com/users/sAws/orgs","repos_url":"https://api.github.com/users/sAws/repos","events_url":"https://api.github.com/users/sAws/events{/privacy}","received_events_url":"https://api.github.com/users/sAws/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2020-02-11T15:36:10Z","updated_at":"2020-02-17T13:53:06Z","closed_at":"2020-02-17T13:53:06Z","author_association":"NONE","active_lock_reason":null,"body":"Hello\r\nI install riak 2.9.0p5\r\nData not created\r\nUse `log.console.level = debug`\r\nAnd i see this error:\r\n`2020-02-11 17:57:10.361 [debug] <0.27846.1>@hashtree:compare:1169 FORMAT ERROR: \"Tree ~p level ~p bucket ~p\\nL=~p\\nR=~p\\nD=\\n\" [{state,<<252,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3>>,1438665674247607560106752257205091097473808596992,3,1048576,1024,0,{dict,0,16,16,8,80,48,{[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]},{{[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]}}},<<>>,\"/var/lib/riak/yz_anti_entropy/1438665674247607560106752257205091097473808596992\",<<>>,incremental,[],0,{array,38837,0,0,100000}},1,0,[],[],[]]`\r\n```\r\nuname -a\r\nLinux riak-1 4.4.0-131-generic #157-Ubuntu SMP Thu Jul 12 15:51:36 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\nlsb_release -a\r\nNo LSB modules are available.\r\nDistributor ID:\tUbuntu\r\nDescription:\tUbuntu 16.04.5 LTS\r\nRelease:\t16.04\r\nCodename:\txenial\r\n```\r\n\r\nFull settings:\r\n```\r\nlog.console = file\r\nlog.console.level = debug\r\nlog.console.file = $(platform_log_dir)/console.log\r\nlog.error.file = $(platform_log_dir)/error.log\r\nlog.syslog = off\r\nlog.crash = on\r\nlog.crash.file = $(platform_log_dir)/crash.log\r\nlog.crash.maximum_message_size = 64KB\r\nlog.crash.size = 10MB\r\nlog.crash.rotation = $D0\r\nlog.crash.rotation.keep = 5\r\nnodename = riak@riak1.local\r\ndistributed_cookie = riak\r\nerlang.async_threads = 64\r\nerlang.max_ports = 262144\r\ndtrace = off\r\nplatform_bin_dir = /usr/sbin\r\nplatform_data_dir = /var/lib/riak\r\nplatform_etc_dir = /etc/riak\r\nplatform_lib_dir = /usr/lib/riak/lib\r\nplatform_log_dir = /var/log/riak\r\nlistener.http.internal = 192.168.65.45:8098\r\nlistener.protobuf.internal = 192.168.65.45:8087\r\nanti_entropy = active\r\ntictacaae_active = passive\r\ntictacaae_dataroot = $(platform_data_dir)/tictac_aae\r\ntictacaae_rebuildwait = 336\r\ntictacaae_rebuilddelay = 345600\r\nnode_worker_pool_size = 2\r\naf1_worker_pool_size = 1\r\naf2_worker_pool_size = 1\r\naf3_worker_pool_size = 2\r\naf4_worker_pool_size = 1\r\nbe_worker_pool_size = 1\r\nstorage_backend = multi\r\nobject.format = 1\r\nobject.size.warning_threshold = 5MB\r\nobject.size.maximum = 50MB\r\nobject.siblings.warning_threshold = 25\r\nobject.siblings.maximum = 100\r\nbitcask.data_root = $(platform_data_dir)/bitcask\r\nbitcask.io_mode = erlang\r\nriak_control = on\r\nriak_control.auth.mode = off\r\nleveldb.maximum_memory.percent = 70\r\nleveldb.compression = on\r\nleveldb.compression.algorithm = lz4\r\nleveled.data_root = $(platform_data_dir)/leveled\r\nleveled.sync_strategy = none\r\nleveled.compression_method = native\r\nleveled.compression_point = on_receipt\r\nleveled.log_level = info\r\nleveled.journal_size = 1000000000\r\nleveled.journal_objectcount = 200000\r\nleveled.ledger_pagecachelevel = 4\r\nleveled.compaction_runs_perday = 24\r\nleveled.compaction_low_hour = 0\r\nleveled.compaction_top_hour = 23\r\nleveled.max_run_length = 4\r\nmulti_backend.leveldb_backend.storage_backend = leveldb\r\nmulti_backend.default = leveldb_backend\r\nmulti_backend.leveldb_backend.leveled.journal_size = 1000000000\r\nmulti_backend.leveldb_backend.leveled.journal_objectcount = 200000\r\nmulti_backend.leveldb_backend.leveled.ledger_pagecachelevel = 4\r\nsearch = on\r\nsearch.solr.start_timeout = 30s\r\nsearch.solr.port = 8093\r\nsearch.solr.jmx_port = 8985\r\nsearch.solr.jvm_options = -d64 -Xms256m -Xmx256m -XX:+UseCompressedOops\r\nerlang.K = on\r\nerlang.W = w\r\nerlang.max_ets_tables = 256000\r\nerlang.smp = enable\r\nerlang.distribution_buffer_size = 131072  # in KB / default = 32MB\r\nerlang.process_limit = 256000\r\nerlang.crash_dump = /var/log/riak/erl_crash.dump\r\nerlang.fullsweep_after = 0\r\nssl.certfile = $(platform_etc_dir)/cert.pem\r\nssl.cacertfile = $(platform_etc_dir)/cert.pem\r\nssl.keyfile = $(platform_etc_dir)/cert.pem\r\n# custom\r\nbuckets.default.n_val = 3\r\nbuckets.default.r = quorum\r\nbuckets.default.w = quorum\r\nbuckets.default.dw = quorum\r\nbuckets.default.rw = all # default quorum\r\nbuckets.default.pw = 1 # default 0\r\nbuckets.default.pr = 1 # default 0\r\nbuckets.default.allow_mult = false\r\nbuckets.default.last_write_wins = true # default false\r\nhandoff.port = 8099\r\nanti_entropy.tree.build_limit.per_timespan = 1000h # default 1h\r\nanti_entropy.tree.build_limit.number = 1\r\nanti_entropy.tree.expiry = 1000w # default 1w\r\nanti_entropy.max_open_files = 20\r\nanti_entropy.write_buffer_size = 4194304 # default 4MB\r\nriak_control.auth.mode = userlist\r\nriak_control.auth.user.dev.password = dev\r\nsasl = off\r\n```\r\nFull logs:\r\n```\r\nA=[]\r\nB=[]\r\nD=[]\r\n2020-02-11 17:56:59.050 [debug] <0.365.0>@riak_kv_entropy_manager:maybe_clear_exchange:609 Untracking exchange: 1164634117248063262943561351070788031288321245184 :: normal\r\n2020-02-11 17:57:10.359 [debug] <0.27838.1>@yz_exchange_fsm:init:87 Starting exchange between KV and Yokozuna: 1438665674247607560106752257205091097473808596992\r\n2020-02-11 17:57:10.359 [debug] <0.3846.1>@riak_kv_index_hashtree:handle_call:344 Updating tree: (vnode)=1438665674247607560106752257205091097473808596992 (preflist)={1438665674247607560106752257205091097473808596992,3}\r\n2020-02-11 17:57:10.359 [debug] <0.2065.0>@yz_solrq_drain_mgr:handle_call:82 Solrq drain starting for partition 1438665674247607560106752257205091097473808596992\r\n2020-02-11 17:57:10.360 [debug] <0.2064.0> Supervisor yz_solrq_sup started yz_solrq_drain_fsm:start_link([{owner_pid,<0.27841.1>},{exchange_fsm_pid,<0.27838.1>},{yz_index_hashtree_update_params,{<0.2777.0>,...}},...]) at pid <0.27842.1>\r\n2020-02-11 17:57:10.360 [debug] <0.27842.1>@yz_solrq_drain_fsm:prepare:165 Solrq drain starting for partition 1438665674247607560106752257205091097473808596992\r\n2020-02-11 17:57:10.360 [debug] <0.27842.1>@yz_solrq_drain_fsm:wait_for_drain_complete:186 Solrq drain completed for all workers for partition 1438665674247607560106752257205091097473808596992.  Resuming batching.\r\n2020-02-11 17:57:10.360 [debug] <0.2777.0>@yz_index_hashtree:handle_call:223 Updating tree for partition 1438665674247607560106752257205091097473808596992 preflist {1438665674247607560106752257205091097473808596992,3}\r\n2020-02-11 17:57:10.360 [debug] <0.27838.1>@yz_exchange_fsm:update_trees:176 Tree yz_index_hashtree built; staying in update_trees state\r\n2020-02-11 17:57:10.360 [debug] <0.27841.1>@yz_solrq_drain_mgr:wait_for_workers_resumed_or_crash:163 Workers resumed.\r\n2020-02-11 17:57:10.361 [debug] <0.27841.1>@yz_solrq_drain_mgr:wait_for_exit:179 Drain <0.27842.1> completed normally.\r\n2020-02-11 17:57:10.361 [debug] <0.27841.1>@yz_solrq_drain_mgr:handle_call:96 Solrq drain about to send compelte message for partition 1438665674247607560106752257205091097473808596992.\r\n2020-02-11 17:57:10.361 [debug] <0.2065.0>@yz_solrq_drain_mgr:handle_cast:105 Solrq drain completed for partition 1438665674247607560106752257205091097473808596992.\r\n2020-02-11 17:57:10.361 [debug] <0.27838.1>@yz_exchange_fsm:update_trees:173 Tree riak_kv_index_hashtree built; Moving to key exchange\r\n2020-02-11 17:57:10.361 [debug] <0.27838.1>@yz_exchange_fsm:key_exchange:184 Starting key exchange for partition 1438665674247607560106752257205091097473808596992 preflist {1438665674247607560106752257205091097473808596992,3}\r\n2020-02-11 17:57:10.361 [debug] <0.27846.1>@hashtree:compare:1169 FORMAT ERROR: \"Tree ~p level ~p bucket ~p\\nL=~p\\nR=~p\\nD=\\n\" [{state,<<252,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3>>,1438665674247607560106752257205091097473808596992,3,1048576,1024,0,{dict,0,16,16,8,80,48,{[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]},{{[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]}}},<<>>,\"/var/lib/riak/yz_anti_entropy/1438665674247607560106752257205091097473808596992\",<<>>,incremental,[],0,{array,38837,0,0,100000}},1,0,[],[],[]]\r\n2020-02-11 17:57:10.361 [debug] <0.2260.0>@yz_entropy_mgr:maybe_clear_exchange:454 Untracking exchange: 1438665674247607560106752257205091097473808596992 :: normal\r\n2020-02-11 17:57:14.043 [debug] <0.27869.1>@riak_kv_exchange_fsm:init:80 Starting exchange: {1164634117248063262943561351070788031288321245184,'riak@riak1.local'}\r\n2020-02-11 17:57:14.044 [info] <0.365.0>@riak_kv_entropy_manager:maybe_exchange:986 Attempt to start exchange between {1164634117248063262943561351070788031288321245184,3} and 1210306043414653979137426502093171875652569137152 resulted in ok\r\n2020-02-11 17:57:14.044 [debug] <0.27869.1>@riak_kv_exchange_fsm:update_trees:151 Sending to {1164634117248063262943561351070788031288321245184,'riak@riak1.local'}\r\n2020-02-11 17:57:14.044 [debug] <0.27869.1>@riak_kv_exchange_fsm:update_trees:152 Sending to {1210306043414653979137426502093171875652569137152,'riak@riak1.local'}\r\n2020-02-11 17:57:14.045 [debug] <0.2980.1>@riak_kv_index_hashtree:handle_call:344 Updating tree: (vnode)=1164634117248063262943561351070788031288321245184 (preflist)={1164634117248063262943561351070788031288321245184,3}\r\n2020-02-11 17:57:14.045 [debug] <0.3116.1>@riak_kv_index_hashtree:handle_call:344 Updating tree: (vnode)=1210306043414653979137426502093171875652569137152 (preflist)={1164634117248063262943561351070788031288321245184,3}\r\n2020-02-11 17:57:14.045 [debug] <0.27869.1>@riak_kv_exchange_fsm:update_trees:166 Moving to key exchange\r\n2020-02-11 17:57:14.045 [debug] <0.27869.1>@riak_kv_exchange_fsm:key_exchange:179 Starting key exchange between {1164634117248063262943561351070788031288321245184,'riak@riak1.local'} and {1210306043414653979137426502093171875652569137152,'riak@riak1.local'}\r\n2020-02-11 17:57:14.045 [debug] <0.27869.1>@riak_kv_exchange_fsm:key_exchange:180 Exchanging hashes for preflist {1164634117248063262943561351070788031288321245184,3}\r\n2020-02-11 17:57:14.047 [debug] <0.27876.1>@hashtree:exchange_level:1132 Exchange Level 1 Bucket 0\r\nA=[]\r\nB=[]\r\nD=[]\r\n```","performed_via_github_app":null,"score":1.0},{"url":"https://api.github.com/repos/basho/riak_kv/issues/1706","repository_url":"https://api.github.com/repos/basho/riak_kv","labels_url":"https://api.github.com/repos/basho/riak_kv/issues/1706/labels{/name}","comments_url":"https://api.github.com/repos/basho/riak_kv/issues/1706/comments","events_url":"https://api.github.com/repos/basho/riak_kv/issues/1706/events","html_url":"https://github.com/basho/riak_kv/issues/1706","id":482040491,"node_id":"MDU6SXNzdWU0ODIwNDA0OTE=","number":1706,"title":"Timer overlaps with hinted handoffs - and Tictac AAE on fallbacks","user":{"login":"martinsumner","id":1628897,"node_id":"MDQ6VXNlcjE2Mjg4OTc=","avatar_url":"https://avatars0.githubusercontent.com/u/1628897?v=4","gravatar_id":"","url":"https://api.github.com/users/martinsumner","html_url":"https://github.com/martinsumner","followers_url":"https://api.github.com/users/martinsumner/followers","following_url":"https://api.github.com/users/martinsumner/following{/other_user}","gists_url":"https://api.github.com/users/martinsumner/gists{/gist_id}","starred_url":"https://api.github.com/users/martinsumner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/martinsumner/subscriptions","organizations_url":"https://api.github.com/users/martinsumner/orgs","repos_url":"https://api.github.com/users/martinsumner/repos","events_url":"https://api.github.com/users/martinsumner/events{/privacy}","received_events_url":"https://api.github.com/users/martinsumner/received_events","type":"User","site_admin":false},"labels":[{"id":1373832059,"node_id":"MDU6TGFiZWwxMzczODMyMDU5","url":"https://api.github.com/repos/basho/riak_kv/labels/2.9%20Known%20Issue","name":"2.9 Known Issue","color":"e87fd4","default":false,"description":"Known issues with the Riak 2.9 release"},{"id":234112,"node_id":"MDU6TGFiZWwyMzQxMTI=","url":"https://api.github.com/repos/basho/riak_kv/labels/Bug","name":"Bug","color":"FF0000","default":false,"description":null}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2019-08-18T21:43:00Z","updated_at":"2020-02-25T10:19:19Z","closed_at":"2020-02-25T10:19:19Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"When there is a node failure/stop event, Riak KV will elect nodes to run fallback vnodes for each partition/preflist for which there was a primary vnode running on the failed vnode.  This normally means `n` fallback vnodes within the cluster for each previously active vnode on the failed/stopped node.  \r\n\r\nWhen the failure state is reverted due to recovery/restart, then those fallback nodes should handoff any data they contain back to the primary they replaced (the `hinted handoff` process).  The data they contain will be both missing PUTs (e.g. PUTs the primary did not receive whilst it was unavailable), plus normally any objects which have been GET during the outage (due to read-repair populating fallbacks, perhaps a matter for separate discussion).\r\n\r\nThere is potential issue with this handoff process when tictac aae is enabled.  The severity of this problem is reduced by the fact that having anti-entropy makes the hinted handoff not strictly necessary (although still potentially helpful).\r\n\r\nThe issue can be seen with handoffs \"stuck\" for 15 minutes after recovery in the following state:\r\n\r\n|         Node         |Total|Ownership|Resize|Hinted|Repair|\r\n|:---------------|:----:|:----:|:----:|:----:|:----:|\r\n|  dev@192.168.143.43  |  0  |         |      |0 (1) |      |\r\n|  dev@192.168.143.44  |  0  |         |      |      |      |\r\n|  dev@192.168.143.45  |  0  |         |      |0 (1) |      |\r\n|  dev@192.168.143.46  |  0  |         |      |      |      |\r\n|  dev@192.168.143.47  |  0  |         |      |      |      |\r\n|  dev@192.168.143.49  |  0  |         |      |      |      |\r\n\r\nThis state indicates that the `riak_core_handoff_manager` is not aware of any active handoffs, but the `riak_core_vnode_manager` is aware of an outstanding handoff that is required to be run - this is called a `known` handoff requirement.\r\n\r\nThere is a single `riak_core_vnode_manager` on each node which is responsible for tracking changes in the `ring` and tracking the need to stop/start vnodes and prompt any required handoffs or repairs.  However, the `riak_core_vnode_manager` does not itself trigger the transfer - when it is aware of the need for a transfer it awaits an `inactive` event to be received from the vnode, that will then trigger the vnode to prompt a required handoff through `maybe_trigger_handoff/4`.  The vnodes have to poll the `riak_core_vnode_manager` when they are inactive, in order to trigger the manager to potentially prompt the same vnode to handoff.\r\n\r\nWhen the vnode then triggers handoff, it does so in negotiation with the `riak_core_handoff_manager` which is controlling the level of concurrent transfers allowed on the node.  When the transfer is complete, the `riak_core_vnode_manager` is informed via  a `handoff_complete` event.  This event will change the handoff from being `known` (hence remove it from the count in parenthesis in the table above).\r\n\r\nWith `tictac_aae` enabled, some fallbacks are not prompting their handoffs for a period of around 15 minutes.  This is because the fallback vnodes are not sending to the `riak_core_vnode_manager` the `inactive` event in the expected time interval.\r\n\r\nThe `inactive` message is prompted by a FSM timeout being triggered on the `riak_core_vnode` - normally set through the `riak_core_vnode:continue/1` function.  This timeout is different for each vnode in that it will be by default 60s plus a further `random:uniform(60)`s.  The design of riak assumes that when a fallback vnode is no longer in use (due to recovery/repair) it will fall inactive - as it will stop receiving GET and PUT requests.\r\n\r\nThe issue with enabling Tictac AAE on a cluster is that Tictac AAE activity is triggered by a `tictacaae_exchangepoke` message on the vnode, and to ensure AAE keeps running the poke messages occur every 120s with the next poke prompted as a `send_after` when the previous poke is received, and before any work has been prompted by the poke.\r\n\r\nWhen there is AAE work to do (following the poke) that work will take o(5)s, and on completion there will be an `exchange_complete` message to the vnode - so during AAE work the vnode is inactive for only ~ __115s__.  After 8 pokes (assuming an n-val of 3) the exchange cycle is complete, and from the next poke the ring is consulted to reloaded a new set of exchanges, and during that reload exchange cycle no exchange is run (and so no follow-up complete message is received) - and so the vnode is inactive for ~ __120s__.\r\n\r\nDue to the random aspect of choosing an inactivity timeout, the `riak_core_vnode` inactivity timer may be between __115s__ and __120s__, although normally it will be below __115s__.  With the higher range of inactivity timeouts the \"stuck\" handoff issue may occur - hence why it only occurs on a small minority of fallback vnodes.  Note that the random choice of timeout extension is made once and only once on initialising the vnode - to save making a `random:uniform/1` call on receipt of every request to the vnode\r\n\r\nThere are two issues to highlight here:\r\n\r\n1. There is an overlap between looping message and the expectation of inactivity, which is complicated by the timing of both the looping message and the inactivity timers being independently configurable  (via `riak_core.vnode_inactivity_timeout` and the `riak_kv.tictacaae_exchangetick`) and partially random.\r\n\r\n2. Why is the Tictac AAE active when a vnode is in fallback mode?  There are two parts to this, being active for building AAE trees, and being active active for exchanges.  Note that part of the problem here is that the ring understands the status of the vnode from the perspective of a given preflist, but the vnode is not started with any awareness of its role.  The vnode knows it is alive, but not why it lives. \r\n\r\n\r\n\r\n","performed_via_github_app":null,"score":1.0},{"url":"https://api.github.com/repos/basho/riak_kv/issues/1714","repository_url":"https://api.github.com/repos/basho/riak_kv","labels_url":"https://api.github.com/repos/basho/riak_kv/issues/1714/labels{/name}","comments_url":"https://api.github.com/repos/basho/riak_kv/issues/1714/comments","events_url":"https://api.github.com/repos/basho/riak_kv/issues/1714/events","html_url":"https://github.com/basho/riak_kv/issues/1714","id":493252567,"node_id":"MDU6SXNzdWU0OTMyNTI1Njc=","number":1714,"title":"Node failure - whether or not to \"let it crash\"","user":{"login":"martinsumner","id":1628897,"node_id":"MDQ6VXNlcjE2Mjg4OTc=","avatar_url":"https://avatars0.githubusercontent.com/u/1628897?v=4","gravatar_id":"","url":"https://api.github.com/users/martinsumner","html_url":"https://github.com/martinsumner","followers_url":"https://api.github.com/users/martinsumner/followers","following_url":"https://api.github.com/users/martinsumner/following{/other_user}","gists_url":"https://api.github.com/users/martinsumner/gists{/gist_id}","starred_url":"https://api.github.com/users/martinsumner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/martinsumner/subscriptions","organizations_url":"https://api.github.com/users/martinsumner/orgs","repos_url":"https://api.github.com/users/martinsumner/repos","events_url":"https://api.github.com/users/martinsumner/events{/privacy}","received_events_url":"https://api.github.com/users/martinsumner/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2019-09-13T10:30:43Z","updated_at":"2020-02-25T10:22:28Z","closed_at":"2020-02-25T10:22:28Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"In order to correctly recover from a failure, `riak_core_node_watcher` must detect that the service is down on a given node.  Without such an event, fallback nodes will not be started, and the vnodes used in requests will not change to reflect those fallback vnodes.\r\n\r\nMy understanding is that there are two triggers for discovering a failed node in `riak_core_node_watcher`:\r\n\r\n- erlang node monitoring through net_kernel\r\n- registering a health_check to test the availability of a particular service on a node\r\n\r\nThe supervision tree for riak_kv will normally attempt to restart individual services that fail, but if they continue to fail with sufficient intensity the failure will ripple up the supervision tree and bring the node down.  This will trigger a `node_down` message for other the riak_core_node_watcher to detect on other nodes.\r\n\r\nAs of, a long time ago, the use of health checks (the second method) was disabled by default in riak_kv: https://github.com/basho/riak_kv/commit/5860d68a8ac7eed3eb33e9346d90b946864a488f.  The actual health check used seems to overlap with overload handling, and it is easy to see how it could have a negative impact with node flapping under load.\r\n\r\nThis then leads to the question:\r\n\r\n- What if a failure can render a riak-kv node inoperable, but there are no crashes in the supervision tree?\r\n\r\nThere appears to have been a case of this with a riak customer, where it appears the file system got switched into a read-only mode on at least one node.  The vnode backend was eleveldb, and the impacted vnodes were responding to PUTs with a [db_write error](https://github.com/basho/eleveldb/blob/develop-2.9/c_src/workitems.cc#L205), but this was returned as an error - it did not cause a crash of either the backend or the vnode.\r\n\r\nAs the node was still \"up\", there had been no crash, the vnodes on the broken node were still being selected to coordinate PUTs.  Any PUT coordinated by a vnode on the failed node would then fail (a failure on a coordinator is immediately sent back to the application as a failure, there is no attempt to try the other n -1 vnodes).  So the application saw an ongoing series of intermittent failures.\r\n\r\nEventually some impacted vnodes ran out of their lease counters, and attempting to renew leases crashed the `vnode_status_manager`, which couldn't write to the file system.  The `riak_kv_vnode` monitors the the `vnode_status_manager`, but responds by restarting it (which doesn't crash), then making an async request to lease another counter (which does crash - and prompts another exit message for the riak_kv_vnode to handle) - https://github.com/basho/riak_kv/blob/riak_kv-2.9.0p5/src/riak_kv_vnode.erl#L2212-L2226.  As this is manually monitored (i.e. not supervised with an intensity check) the vnode vnode_status_manager entered a perpetual loop or crashes and restarts, without crashing the vnode.\r\n\r\nThere is a similar issue with the hashtree process, which crash, but enter a perpetual loop of restarting on detection of the linked loop going down, without ever failing the vnode - https://github.com/basho/riak_kv/blob/riak_kv-2.9.0p5/src/riak_kv_vnode.erl#L282-L286.\r\n\r\nThe net effect of all of this is the cluster not doing its job.  One or more nodes became unusable, without other nodes taking action to recover from this.  But what is the fault here:\r\n\r\n- Should eleveldb have crashed on the db_write error, rather than returning {error, Reason}?  \r\n  - Although if it did, given the immediate impact of a NIF crash, is this too draconian a response?  Perhaps the `riak_kv_eleveldb_backend` should detect this as a bad case and crash?\r\n\r\n- Should the riak_kv_vnode crash on receiving an error from a co-ordinated PUT attempt?\r\n\r\n- Should the PUT_FSM select another coordinator if the original one returns an error?\r\n  - At least then a single node failing under these conditions would not lead to actual write errors.\r\n  - The customer was running Riak KV 2.2.0, there may have been some assistance here in running the soft vnode queue checks in Riak KV 2.9.0. \r\n\r\n- Should there be some pseudo intensity monitoring on restarts of hashtrees and vnode_status_manager to crash the vnode?\r\n  - It should be noted that if in this case the vnode would have crashed once, it would have crashed immediately on startup, so an appropriate and immediate cascade up the supervision tree would have occurred.\r\n\r\n- Should a better riak_kv health_check be used (one which performed a more genuine check of vnode health), rather than simply disabling the health checks?\r\n\r\n- Is my understanding incomplete?  Are there other mechanisms for detecting node failure (other than outright crash or the disabled health check) that should have prompted recovery from this failure scenario.\r\n\r\n \r\n\r\n\r\n","performed_via_github_app":null,"score":1.0},{"url":"https://api.github.com/repos/heimrichhannot/contao-news-pagination-bundle/issues/2","repository_url":"https://api.github.com/repos/heimrichhannot/contao-news-pagination-bundle","labels_url":"https://api.github.com/repos/heimrichhannot/contao-news-pagination-bundle/issues/2/labels{/name}","comments_url":"https://api.github.com/repos/heimrichhannot/contao-news-pagination-bundle/issues/2/comments","events_url":"https://api.github.com/repos/heimrichhannot/contao-news-pagination-bundle/issues/2/events","html_url":"https://github.com/heimrichhannot/contao-news-pagination-bundle/issues/2","id":419072205,"node_id":"MDU6SXNzdWU0MTkwNzIyMDU=","number":2,"title":"Instalerror with php 7.2.15","user":{"login":"harley-rider","id":18749372,"node_id":"MDQ6VXNlcjE4NzQ5Mzcy","avatar_url":"https://avatars1.githubusercontent.com/u/18749372?v=4","gravatar_id":"","url":"https://api.github.com/users/harley-rider","html_url":"https://github.com/harley-rider","followers_url":"https://api.github.com/users/harley-rider/followers","following_url":"https://api.github.com/users/harley-rider/following{/other_user}","gists_url":"https://api.github.com/users/harley-rider/gists{/gist_id}","starred_url":"https://api.github.com/users/harley-rider/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/harley-rider/subscriptions","organizations_url":"https://api.github.com/users/harley-rider/orgs","repos_url":"https://api.github.com/users/harley-rider/repos","events_url":"https://api.github.com/users/harley-rider/events{/privacy}","received_events_url":"https://api.github.com/users/harley-rider/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":7,"created_at":"2019-03-09T13:34:34Z","updated_at":"2020-04-01T06:31:12Z","closed_at":"2020-04-01T06:31:12Z","author_association":"NONE","active_lock_reason":null,"body":"I got this error with adding the bundle to cto 4.4.35\r\n\r\nFatal error: Cannot use Riak\\Object as Object because 'Object' is a special class name in I:\\xampp-7-2-15-2-x64\\htdocs\\contao-4-4-lts\\hr-vision\\vendor\\doctrine\\cache\\lib\\Doctrine\\Common\\Cache\\RiakCache.php on line 8\r\nPHP Fatal error:  Cannot use Riak\\Object as Object because 'Object' is a special class name in I:\\xampp-7-2-15-2-x64\\htdocs\\contao-4-4-lts\\hr-vision\\vendor\\doctrine\\cache\\lib\\Doctrine\\Common\\Cache\\RiakCache.php on line 8\r\nScript Contao\\ManagerBundle\\Composer\\ScriptHandler::initializeApplication handling the post-install-cmd event terminated with an exception\r\n\r\nIn ScriptHandler.php line 113:\r\n  An error occurred while executing the \"contao:install-web-dir\" command: PHP  \r\n   Fatal error:  Cannot use Riak\\Object as Object because 'Object' is a speci  \r\n  al class name in I:\\xampp-7-2-15-2-x64\\htdocs\\contao-4-4-lts\\hr-vision\\vend  \r\n  or\\doctrine\\cache\\lib\\Doctrine\\Common\\Cache\\RiakCache.php on line 8","performed_via_github_app":null,"score":1.0},{"url":"https://api.github.com/repos/basho/riak_kv/issues/1750","repository_url":"https://api.github.com/repos/basho/riak_kv","labels_url":"https://api.github.com/repos/basho/riak_kv/issues/1750/labels{/name}","comments_url":"https://api.github.com/repos/basho/riak_kv/issues/1750/comments","events_url":"https://api.github.com/repos/basho/riak_kv/issues/1750/events","html_url":"https://github.com/basho/riak_kv/issues/1750","id":566758397,"node_id":"MDU6SXNzdWU1NjY3NTgzOTc=","number":1750,"title":"node_confirms on GET","user":{"login":"martinsumner","id":1628897,"node_id":"MDQ6VXNlcjE2Mjg4OTc=","avatar_url":"https://avatars0.githubusercontent.com/u/1628897?v=4","gravatar_id":"","url":"https://api.github.com/users/martinsumner","html_url":"https://github.com/martinsumner","followers_url":"https://api.github.com/users/martinsumner/followers","following_url":"https://api.github.com/users/martinsumner/following{/other_user}","gists_url":"https://api.github.com/users/martinsumner/gists{/gist_id}","starred_url":"https://api.github.com/users/martinsumner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/martinsumner/subscriptions","organizations_url":"https://api.github.com/users/martinsumner/orgs","repos_url":"https://api.github.com/users/martinsumner/repos","events_url":"https://api.github.com/users/martinsumner/events{/privacy}","received_events_url":"https://api.github.com/users/martinsumner/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2020-02-18T09:29:27Z","updated_at":"2020-02-25T10:18:01Z","closed_at":"2020-02-25T10:18:01Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"The node_confirms bucket property is respected on PUT.  A PUT may succeed (in that it has been written to Riak) but will error when insufficient vnodes have responded from different nodes.  This allows us to be sure that the PUT will not disappear on a single node failure.\r\n\r\nMore information - https://github.com/basho/riak_kv/blob/develop-2.9/docs/Node-Diversity.md\r\n\r\nIf a PUT is made and errors due to node_confirms.  After the PUT, a GET can be made and succeed - but at that stage, the object may still be on a single node.  How does the application know if it has safely stored the data on two nodes - especially as this may now be a different process reading the object, and so not be aware of the potentially failed PUT i.e. re-PUT until a success response is not an option.\r\n\r\nIt would be useful to have a GET request, where success indicates that even on a single node failure, it can be safely assumed that the read change will not be lost from the history.  \r\n\r\nPR=2 can be used as an alternative, but this will fail in lots of cases when node diversity does exist.  The other issue with PR=2 and PW=2 is when a node is started, but not joined to the cluster, and perhaps due to admin error is introduced into a load-balanced group.  Now PR=2, and PW=2 will succeed, but we haven't actually ensured the data is stored in more than one place.\r\n\r\nIt is possible to add node_confirms to GET.  However, this would not naturally confirm that the PUT has reached two nodes - counting parameter on GETs count the number of nodes consulted, but the answer may still only come from one node.  However, if node_confirms=2 on GET, on success the application would know that:\r\n\r\n- More than one node in the preflist is now active;\r\n- That if the returned record is not on one node, a read repair is pending to fix that.\r\n\r\nThe cluster using node_confirms = 2 on GET and PUT would still have a degree of partition tolerance.  however, some preflists on some partitions (especially minority partitions) may not have node diversity and would error.\r\n","performed_via_github_app":null,"score":1.0},{"url":"https://api.github.com/repos/basho/riak_kv/issues/1691","repository_url":"https://api.github.com/repos/basho/riak_kv","labels_url":"https://api.github.com/repos/basho/riak_kv/issues/1691/labels{/name}","comments_url":"https://api.github.com/repos/basho/riak_kv/issues/1691/comments","events_url":"https://api.github.com/repos/basho/riak_kv/issues/1691/events","html_url":"https://github.com/basho/riak_kv/issues/1691","id":425343901,"node_id":"MDU6SXNzdWU0MjUzNDM5MDE=","number":1691,"title":"Automated full-sync with Tictac AAE","user":{"login":"martinsumner","id":1628897,"node_id":"MDQ6VXNlcjE2Mjg4OTc=","avatar_url":"https://avatars0.githubusercontent.com/u/1628897?v=4","gravatar_id":"","url":"https://api.github.com/users/martinsumner","html_url":"https://github.com/martinsumner","followers_url":"https://api.github.com/users/martinsumner/followers","following_url":"https://api.github.com/users/martinsumner/following{/other_user}","gists_url":"https://api.github.com/users/martinsumner/gists{/gist_id}","starred_url":"https://api.github.com/users/martinsumner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/martinsumner/subscriptions","organizations_url":"https://api.github.com/users/martinsumner/orgs","repos_url":"https://api.github.com/users/martinsumner/repos","events_url":"https://api.github.com/users/martinsumner/events{/privacy}","received_events_url":"https://api.github.com/users/martinsumner/received_events","type":"User","site_admin":false},"labels":[{"id":1290152206,"node_id":"MDU6TGFiZWwxMjkwMTUyMjA2","url":"https://api.github.com/repos/basho/riak_kv/labels/2.9.1","name":"2.9.1","color":"bfdadc","default":false,"description":"Extension to release 2.9"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":21,"created_at":"2019-03-26T10:43:36Z","updated_at":"2020-02-25T10:21:33Z","closed_at":"2020-02-25T10:21:32Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Tictac AAE was introduced in 2.9.0, which allowed for the use of AAE in full-sync co-ordination between clusters with:\r\n\r\n- no pausing for (or failures due to) rebuilds;\r\n- support for full-sync between different ring_size;\r\n- support for per-bucket full-sync;\r\n- support for use of cached entropy trees (all bucket full-sync) or modified date range full-sync (per bucket) to improve efficiency.\r\n\r\nHowever, using this feature requires the customer to control and manage the full-sync process using the `kv_index_tictactree` `aae_exchange` module, and the `aae_fold` api.  There was no default, automated way of adding full-sync between clusters through configuration.\r\n\r\nThis issue is a feature request to provide the default automated full-sync through configuration.","performed_via_github_app":null,"score":1.0},{"url":"https://api.github.com/repos/basho/riak_kv/issues/1719","repository_url":"https://api.github.com/repos/basho/riak_kv","labels_url":"https://api.github.com/repos/basho/riak_kv/issues/1719/labels{/name}","comments_url":"https://api.github.com/repos/basho/riak_kv/issues/1719/comments","events_url":"https://api.github.com/repos/basho/riak_kv/issues/1719/events","html_url":"https://github.com/basho/riak_kv/issues/1719","id":500784052,"node_id":"MDU6SXNzdWU1MDA3ODQwNTI=","number":1719,"title":"HEAD and multi-backend","user":{"login":"martinsumner","id":1628897,"node_id":"MDQ6VXNlcjE2Mjg4OTc=","avatar_url":"https://avatars0.githubusercontent.com/u/1628897?v=4","gravatar_id":"","url":"https://api.github.com/users/martinsumner","html_url":"https://github.com/martinsumner","followers_url":"https://api.github.com/users/martinsumner/followers","following_url":"https://api.github.com/users/martinsumner/following{/other_user}","gists_url":"https://api.github.com/users/martinsumner/gists{/gist_id}","starred_url":"https://api.github.com/users/martinsumner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/martinsumner/subscriptions","organizations_url":"https://api.github.com/users/martinsumner/orgs","repos_url":"https://api.github.com/users/martinsumner/repos","events_url":"https://api.github.com/users/martinsumner/events{/privacy}","received_events_url":"https://api.github.com/users/martinsumner/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2019-10-01T09:46:59Z","updated_at":"2019-12-16T09:53:06Z","closed_at":"2019-12-16T09:53:06Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"If there is a multi-backend, the HEAD capability is disabled.  The riak_kv_vnode should ask for the capability on a per-bucket basis - as this would allow HEAD to be supported when the bucket is mapped to a leveled backend","performed_via_github_app":null,"score":1.0},{"url":"https://api.github.com/repos/basho/riak_core/issues/943","repository_url":"https://api.github.com/repos/basho/riak_core","labels_url":"https://api.github.com/repos/basho/riak_core/issues/943/labels{/name}","comments_url":"https://api.github.com/repos/basho/riak_core/issues/943/comments","events_url":"https://api.github.com/repos/basho/riak_core/issues/943/events","html_url":"https://github.com/basho/riak_core/issues/943","id":513032835,"node_id":"MDU6SXNzdWU1MTMwMzI4MzU=","number":943,"title":"riak_core_claimant tick can occur after close","user":{"login":"martinsumner","id":1628897,"node_id":"MDQ6VXNlcjE2Mjg4OTc=","avatar_url":"https://avatars0.githubusercontent.com/u/1628897?v=4","gravatar_id":"","url":"https://api.github.com/users/martinsumner","html_url":"https://github.com/martinsumner","followers_url":"https://api.github.com/users/martinsumner/followers","following_url":"https://api.github.com/users/martinsumner/following{/other_user}","gists_url":"https://api.github.com/users/martinsumner/gists{/gist_id}","starred_url":"https://api.github.com/users/martinsumner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/martinsumner/subscriptions","organizations_url":"https://api.github.com/users/martinsumner/orgs","repos_url":"https://api.github.com/users/martinsumner/repos","events_url":"https://api.github.com/users/martinsumner/events{/privacy}","received_events_url":"https://api.github.com/users/martinsumner/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2019-10-27T22:39:26Z","updated_at":"2019-11-05T10:26:35Z","closed_at":"2019-11-05T10:26:34Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"When a node is leaving a cluster the final trigger to exit is a call to `riak_core_ring_manager:refresh_my_ring/0`\r\n\r\nhttps://github.com/basho/riak_core/blob/9f7d0538e01e248dc4f5e0efff328737f23d8fb9/src/riak_core_ring_manager.erl#L377-L388\r\n\r\nThe refresh_my_ring call will be prompted from `riak_core_ring_handler:ensure_vnodes_started/1`:\r\n\r\nhttps://github.com/basho/riak_core/blob/9f7d0538e01e248dc4f5e0efff328737f23d8fb9/src/riak_core_ring_handler.erl#L61-L99\r\n\r\nThis will in turn be prompted by the management tick in the `riak_core_vnode_manager`.\r\n\r\nThe `refresh_my_ring` logic will generate a fresh ring, persist, before prompting riak_core to close.\r\n\r\nHowever, this will prompt the close process to start a structured shutdown of the application, and that shutdown might not be immediate.  As it is not immediate, before the shutdown is complete the next tick event may occur on the `riak_core_claimant`:\r\n\r\nhttps://github.com/basho/riak_core/blob/9f7d0538e01e248dc4f5e0efff328737f23d8fb9/src/riak_core_claimant.erl#L632-L644\r\n\r\nThis can create an issue for `riak_ensemble`.  `riak_ensemble` is dependent on certain changes to the root ensemble being prompted by the claimant (a singleton claimant within the cluster). \r\n\r\nhttps://github.com/basho/riak_core/blob/9f7d0538e01e248dc4f5e0efff328737f23d8fb9/src/riak_core_claimant.erl#L830-L863\r\n\r\nIf the single to claimant is a node which has been set to `leave`, then we need that claimant node to prompt the removal changes to the root ensemble.  However, if the schedule tick required to make this happen occurs after the ring has been refreshed - this may lead to a bad situation.  In this case the refreshed ring will not have the local node() removed, but instead the local node() will be the only member of the ring.\r\n\r\nNow when bootstrapping the root_ensemble as a result of the tick, the code will see all members of the root ensemble other than the local node as having been removed - and will spawn a process to prompt their removal.\r\n\r\nhttps://github.com/basho/riak_core/blob/9f7d0538e01e248dc4f5e0efff328737f23d8fb9/src/riak_core_claimant.erl#L861\r\n\r\nThis is erroneous - in fact the local node is the only one which should be removed.\r\n\r\nThis leads to an intermittent failure of the `riak_test` `ensemble_remove_node` (which can made to fail fairly reliably by increasing timeouts).  It fails as the `riak_ensemble_manager:cluster/0` has no members.\r\n\r\nhttps://github.com/basho/riak_test/blob/8fa9c836a98871d0982f9286cab8d94445129ce5/tests/ensemble_remove_node.erl#L70\r\n\r\nThe precise implications of this is unclear - but this doesn't appear to heal naturally - the cluster state remains incorrect. \r\n\r\n","performed_via_github_app":null,"score":1.0},{"url":"https://api.github.com/repos/basho/riak_ee-issues/issues/17","repository_url":"https://api.github.com/repos/basho/riak_ee-issues","labels_url":"https://api.github.com/repos/basho/riak_ee-issues/issues/17/labels{/name}","comments_url":"https://api.github.com/repos/basho/riak_ee-issues/issues/17/comments","events_url":"https://api.github.com/repos/basho/riak_ee-issues/issues/17/events","html_url":"https://github.com/basho/riak_ee-issues/issues/17","id":40520705,"node_id":"MDU6SXNzdWU0MDUyMDcwNQ==","number":17,"title":"replServerTXRateTable and replServerRXRateTable missing from SNMP output","user":{"login":"angrycub","id":464492,"node_id":"MDQ6VXNlcjQ2NDQ5Mg==","avatar_url":"https://avatars2.githubusercontent.com/u/464492?v=4","gravatar_id":"","url":"https://api.github.com/users/angrycub","html_url":"https://github.com/angrycub","followers_url":"https://api.github.com/users/angrycub/followers","following_url":"https://api.github.com/users/angrycub/following{/other_user}","gists_url":"https://api.github.com/users/angrycub/gists{/gist_id}","starred_url":"https://api.github.com/users/angrycub/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/angrycub/subscriptions","organizations_url":"https://api.github.com/users/angrycub/orgs","repos_url":"https://api.github.com/users/angrycub/repos","events_url":"https://api.github.com/users/angrycub/events{/privacy}","received_events_url":"https://api.github.com/users/angrycub/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2014-08-18T19:05:10Z","updated_at":"2019-11-05T14:22:56Z","closed_at":"2019-11-05T14:22:56Z","author_association":"NONE","active_lock_reason":null,"body":"This is addressed in internal PR https://github.com/basho/riak_snmp/pull/19.  Not yet merged.\n","performed_via_github_app":null,"score":1.0},{"url":"https://api.github.com/repos/basho/riak_core/issues/565","repository_url":"https://api.github.com/repos/basho/riak_core","labels_url":"https://api.github.com/repos/basho/riak_core/issues/565/labels{/name}","comments_url":"https://api.github.com/repos/basho/riak_core/issues/565/comments","events_url":"https://api.github.com/repos/basho/riak_core/issues/565/events","html_url":"https://github.com/basho/riak_core/issues/565","id":30439088,"node_id":"MDU6SXNzdWUzMDQzOTA4OA==","number":565,"title":"Error text for the transfers fun failing is confusing and scary","user":{"login":"angrycub","id":464492,"node_id":"MDQ6VXNlcjQ2NDQ5Mg==","avatar_url":"https://avatars2.githubusercontent.com/u/464492?v=4","gravatar_id":"","url":"https://api.github.com/users/angrycub","html_url":"https://github.com/angrycub","followers_url":"https://api.github.com/users/angrycub/followers","following_url":"https://api.github.com/users/angrycub/following{/other_user}","gists_url":"https://api.github.com/users/angrycub/gists{/gist_id}","starred_url":"https://api.github.com/users/angrycub/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/angrycub/subscriptions","organizations_url":"https://api.github.com/users/angrycub/orgs","repos_url":"https://api.github.com/users/angrycub/repos","events_url":"https://api.github.com/users/angrycub/events{/privacy}","received_events_url":"https://api.github.com/users/angrycub/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":{"url":"https://api.github.com/repos/basho/riak_core/milestones/8","html_url":"https://github.com/basho/riak_core/milestone/8","labels_url":"https://api.github.com/repos/basho/riak_core/milestones/8/labels","id":605232,"node_id":"MDk6TWlsZXN0b25lNjA1MjMy","number":8,"title":"2.0.1","description":"Riak 2.0.1 point release","creator":{"login":"borshop","id":6749250,"node_id":"MDQ6VXNlcjY3NDkyNTA=","avatar_url":"https://avatars0.githubusercontent.com/u/6749250?v=4","gravatar_id":"","url":"https://api.github.com/users/borshop","html_url":"https://github.com/borshop","followers_url":"https://api.github.com/users/borshop/followers","following_url":"https://api.github.com/users/borshop/following{/other_user}","gists_url":"https://api.github.com/users/borshop/gists{/gist_id}","starred_url":"https://api.github.com/users/borshop/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/borshop/subscriptions","organizations_url":"https://api.github.com/users/borshop/orgs","repos_url":"https://api.github.com/users/borshop/repos","events_url":"https://api.github.com/users/borshop/events{/privacy}","received_events_url":"https://api.github.com/users/borshop/received_events","type":"User","site_admin":false},"open_issues":21,"closed_issues":11,"state":"open","created_at":"2014-03-21T17:40:37Z","updated_at":"2019-11-05T14:21:51Z","due_on":"2014-06-20T07:00:00Z","closed_at":null},"comments":1,"created_at":"2014-03-29T07:23:10Z","updated_at":"2019-11-05T14:21:51Z","closed_at":"2019-11-05T14:21:50Z","author_association":"NONE","active_lock_reason":null,"body":"The text at https://github.com/basho/riak_core/blob/develop/src/riak_core_console.erl#L223-L225 can easily be taken to mean that transfers failed rather than the transfers function failing. This confuses and alarms users needlessly.\n","performed_via_github_app":null,"score":1.0},{"url":"https://api.github.com/repos/martinsumner/leveled/issues/301","repository_url":"https://api.github.com/repos/martinsumner/leveled","labels_url":"https://api.github.com/repos/martinsumner/leveled/issues/301/labels{/name}","comments_url":"https://api.github.com/repos/martinsumner/leveled/issues/301/comments","events_url":"https://api.github.com/repos/martinsumner/leveled/issues/301/events","html_url":"https://github.com/martinsumner/leveled/issues/301","id":525008887,"node_id":"MDU6SXNzdWU1MjUwMDg4ODc=","number":301,"title":"Inker Manifest - lacks protection from corruption","user":{"login":"martinsumner","id":1628897,"node_id":"MDQ6VXNlcjE2Mjg4OTc=","avatar_url":"https://avatars0.githubusercontent.com/u/1628897?v=4","gravatar_id":"","url":"https://api.github.com/users/martinsumner","html_url":"https://github.com/martinsumner","followers_url":"https://api.github.com/users/martinsumner/followers","following_url":"https://api.github.com/users/martinsumner/following{/other_user}","gists_url":"https://api.github.com/users/martinsumner/gists{/gist_id}","starred_url":"https://api.github.com/users/martinsumner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/martinsumner/subscriptions","organizations_url":"https://api.github.com/users/martinsumner/orgs","repos_url":"https://api.github.com/users/martinsumner/repos","events_url":"https://api.github.com/users/martinsumner/events{/privacy}","received_events_url":"https://api.github.com/users/martinsumner/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2019-11-19T13:27:13Z","updated_at":"2019-11-19T21:47:49Z","closed_at":"2019-11-19T21:47:48Z","author_association":"OWNER","active_lock_reason":null,"body":"The leveled_imanifest does not read the file back after writing it, before switching it to be the active file.  This means that a poorly timed crash can leave an empty active manifest file:\r\n\r\nhttps://github.com/martinsumner/leveled/blob/f907fb5c97c7f9bee4af5d2bd975b1176992f842/src/leveled_imanifest.erl#L166-L177\r\n\r\nThe rename accepts the `ok` as proof that the file is written.  It should check by reading the file.\r\n\r\nThis can then lead to this on restart:\r\n\r\n```\r\n<0.909.0>@riak_kv_vnode:init:806 Failed to start riak_kv_leveled_backend backend for index 0 crash: {{badmatch,{error,{{badmatch,{error,{badarg,[{erlang,binary_to_term,[<<>>],[]},{leveled_imanifest,reader,2,[{file,\"src/leveled_imanifest.erl\"},{line,160}]},{leveled_inker,build_manifest,3,[{file,\"src/leveled_inker.erl\"},{line,1009}]},{leveled_inker,start_from_file,1,[{file,\"src/leveled_inker.erl\"},{line,834}]},{gen_server,init_it,6,[{file,\"gen_server.erl\"},{line,304}]},{proc_lib,init_p_do_apply,3,[{file,\"proc_lib.erl\"},{line,239}]}]}}},[{leveled_bookie,startup,3,[{file,\"src/leveled_bookie.erl\"},{line,1641}]},{leveled_bookie,init,1,[{file,\"src/leveled_bookie.erl\"},{line,1221}]},{gen_server,init_it,6,[{file,\"gen_server.erl\"},{line,304}]},{proc_lib,init_p_do_apply,3,[{file,\"proc_lib.erl\"},{line,239}]}]}}},[{riak_kv_leveled_backend,start,2,[{file,\"src/riak_kv_leveled_backend.erl\"},{line,148}]},{riak_kv_vnode,init,1,[{file,\"src/riak_kv_vnode.erl\"},{line,763}]},{riak_core_vnode,do_init,1,[{file,\"src/riak_core_vnode.erl\"},{line,224}]},{riak_core_vnode,started,2,[{file,\"src/riak_core_vnode.erl\"},{line,207}]},{gen_fsm,handle_msg,7,[{file,\"gen_fsm.erl\"},{line,505}]},{proc_lib,init_p_do_apply,3,[{file,\"proc_lib.erl\"},{line,239}]}]}\r\n``` \r\n\r\nThis can be resolved by removing the empty file.\r\n\r\nHowever, should we have greater protection against this (e.g. read before rename)?  Should the handling of corruption be better automated? ","performed_via_github_app":null,"score":1.0},{"url":"https://api.github.com/repos/basho/riak_core/issues/939","repository_url":"https://api.github.com/repos/basho/riak_core","labels_url":"https://api.github.com/repos/basho/riak_core/issues/939/labels{/name}","comments_url":"https://api.github.com/repos/basho/riak_core/issues/939/comments","events_url":"https://api.github.com/repos/basho/riak_core/issues/939/events","html_url":"https://github.com/basho/riak_core/issues/939","id":442615354,"node_id":"MDU6SXNzdWU0NDI2MTUzNTQ=","number":939,"title":"Result of new ModState ignored","user":{"login":"ThomasArts","id":638015,"node_id":"MDQ6VXNlcjYzODAxNQ==","avatar_url":"https://avatars1.githubusercontent.com/u/638015?v=4","gravatar_id":"","url":"https://api.github.com/users/ThomasArts","html_url":"https://github.com/ThomasArts","followers_url":"https://api.github.com/users/ThomasArts/followers","following_url":"https://api.github.com/users/ThomasArts/following{/other_user}","gists_url":"https://api.github.com/users/ThomasArts/gists{/gist_id}","starred_url":"https://api.github.com/users/ThomasArts/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ThomasArts/subscriptions","organizations_url":"https://api.github.com/users/ThomasArts/orgs","repos_url":"https://api.github.com/users/ThomasArts/repos","events_url":"https://api.github.com/users/ThomasArts/events{/privacy}","received_events_url":"https://api.github.com/users/ThomasArts/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":{"login":"martinsumner","id":1628897,"node_id":"MDQ6VXNlcjE2Mjg4OTc=","avatar_url":"https://avatars0.githubusercontent.com/u/1628897?v=4","gravatar_id":"","url":"https://api.github.com/users/martinsumner","html_url":"https://github.com/martinsumner","followers_url":"https://api.github.com/users/martinsumner/followers","following_url":"https://api.github.com/users/martinsumner/following{/other_user}","gists_url":"https://api.github.com/users/martinsumner/gists{/gist_id}","starred_url":"https://api.github.com/users/martinsumner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/martinsumner/subscriptions","organizations_url":"https://api.github.com/users/martinsumner/orgs","repos_url":"https://api.github.com/users/martinsumner/repos","events_url":"https://api.github.com/users/martinsumner/events{/privacy}","received_events_url":"https://api.github.com/users/martinsumner/received_events","type":"User","site_admin":false},"assignees":[{"login":"martinsumner","id":1628897,"node_id":"MDQ6VXNlcjE2Mjg4OTc=","avatar_url":"https://avatars0.githubusercontent.com/u/1628897?v=4","gravatar_id":"","url":"https://api.github.com/users/martinsumner","html_url":"https://github.com/martinsumner","followers_url":"https://api.github.com/users/martinsumner/followers","following_url":"https://api.github.com/users/martinsumner/following{/other_user}","gists_url":"https://api.github.com/users/martinsumner/gists{/gist_id}","starred_url":"https://api.github.com/users/martinsumner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/martinsumner/subscriptions","organizations_url":"https://api.github.com/users/martinsumner/orgs","repos_url":"https://api.github.com/users/martinsumner/repos","events_url":"https://api.github.com/users/martinsumner/events{/privacy}","received_events_url":"https://api.github.com/users/martinsumner/received_events","type":"User","site_admin":false}],"milestone":null,"comments":2,"created_at":"2019-05-10T09:06:20Z","updated_at":"2019-10-27T22:12:32Z","closed_at":"2019-10-27T22:12:32Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"This looks like a harmless unused variable, but ModState0 may indicate a new workerpool added that is \"forgotten\" about in the system.\r\n\r\nAs a consequence, in some settings, `queue_work` may fail unexpectedly\r\n\r\nhttps://github.com/basho/riak_core/blob/22b92dc14dc47a7f7f57bf642a45adc791bb9234/src/riak_core_vnode.erl#L233","performed_via_github_app":null,"score":1.0},{"url":"https://api.github.com/repos/doctrine/cache/issues/206","repository_url":"https://api.github.com/repos/doctrine/cache","labels_url":"https://api.github.com/repos/doctrine/cache/issues/206/labels{/name}","comments_url":"https://api.github.com/repos/doctrine/cache/issues/206/comments","events_url":"https://api.github.com/repos/doctrine/cache/issues/206/events","html_url":"https://github.com/doctrine/cache/issues/206","id":198747359,"node_id":"MDU6SXNzdWUxOTg3NDczNTk=","number":206,"title":"RiakCache: rewrite to use native PHP riak client","user":{"login":"robocoder","id":922051,"node_id":"MDQ6VXNlcjkyMjA1MQ==","avatar_url":"https://avatars1.githubusercontent.com/u/922051?v=4","gravatar_id":"","url":"https://api.github.com/users/robocoder","html_url":"https://github.com/robocoder","followers_url":"https://api.github.com/users/robocoder/followers","following_url":"https://api.github.com/users/robocoder/following{/other_user}","gists_url":"https://api.github.com/users/robocoder/gists{/gist_id}","starred_url":"https://api.github.com/users/robocoder/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/robocoder/subscriptions","organizations_url":"https://api.github.com/users/robocoder/orgs","repos_url":"https://api.github.com/users/robocoder/repos","events_url":"https://api.github.com/users/robocoder/events{/privacy}","received_events_url":"https://api.github.com/users/robocoder/received_events","type":"User","site_admin":false},"labels":[{"id":24486530,"node_id":"MDU6TGFiZWwyNDQ4NjUzMA==","url":"https://api.github.com/repos/doctrine/cache/labels/Won't%20Fix","name":"Won't Fix","color":"ffffff","default":false,"description":""}],"state":"closed","locked":false,"assignee":{"login":"Ocramius","id":154256,"node_id":"MDQ6VXNlcjE1NDI1Ng==","avatar_url":"https://avatars2.githubusercontent.com/u/154256?v=4","gravatar_id":"","url":"https://api.github.com/users/Ocramius","html_url":"https://github.com/Ocramius","followers_url":"https://api.github.com/users/Ocramius/followers","following_url":"https://api.github.com/users/Ocramius/following{/other_user}","gists_url":"https://api.github.com/users/Ocramius/gists{/gist_id}","starred_url":"https://api.github.com/users/Ocramius/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Ocramius/subscriptions","organizations_url":"https://api.github.com/users/Ocramius/orgs","repos_url":"https://api.github.com/users/Ocramius/repos","events_url":"https://api.github.com/users/Ocramius/events{/privacy}","received_events_url":"https://api.github.com/users/Ocramius/received_events","type":"User","site_admin":false},"assignees":[{"login":"Ocramius","id":154256,"node_id":"MDQ6VXNlcjE1NDI1Ng==","avatar_url":"https://avatars2.githubusercontent.com/u/154256?v=4","gravatar_id":"","url":"https://api.github.com/users/Ocramius","html_url":"https://github.com/Ocramius","followers_url":"https://api.github.com/users/Ocramius/followers","following_url":"https://api.github.com/users/Ocramius/following{/other_user}","gists_url":"https://api.github.com/users/Ocramius/gists{/gist_id}","starred_url":"https://api.github.com/users/Ocramius/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Ocramius/subscriptions","organizations_url":"https://api.github.com/users/Ocramius/orgs","repos_url":"https://api.github.com/users/Ocramius/repos","events_url":"https://api.github.com/users/Ocramius/events{/privacy}","received_events_url":"https://api.github.com/users/Ocramius/received_events","type":"User","site_admin":false}],"milestone":{"url":"https://api.github.com/repos/doctrine/cache/milestones/22","html_url":"https://github.com/doctrine/cache/milestone/22","labels_url":"https://api.github.com/repos/doctrine/cache/milestones/22/labels","id":4833432,"node_id":"MDk6TWlsZXN0b25lNDgzMzQzMg==","number":22,"title":"1.10.0","description":"This release drops support for the Riak driver, as it was never supported on PHP 7 and caused issues due to the reserved `object` keyword. This library now officially supports PHP 7.4.","creator":{"login":"Ocramius","id":154256,"node_id":"MDQ6VXNlcjE1NDI1Ng==","avatar_url":"https://avatars2.githubusercontent.com/u/154256?v=4","gravatar_id":"","url":"https://api.github.com/users/Ocramius","html_url":"https://github.com/Ocramius","followers_url":"https://api.github.com/users/Ocramius/followers","following_url":"https://api.github.com/users/Ocramius/following{/other_user}","gists_url":"https://api.github.com/users/Ocramius/gists{/gist_id}","starred_url":"https://api.github.com/users/Ocramius/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Ocramius/subscriptions","organizations_url":"https://api.github.com/users/Ocramius/orgs","repos_url":"https://api.github.com/users/Ocramius/repos","events_url":"https://api.github.com/users/Ocramius/events{/privacy}","received_events_url":"https://api.github.com/users/Ocramius/received_events","type":"User","site_admin":false},"open_issues":0,"closed_issues":13,"state":"closed","created_at":"2019-11-11T10:23:55Z","updated_at":"2019-11-29T15:36:52Z","due_on":null,"closed_at":"2019-11-29T15:36:52Z"},"comments":9,"created_at":"2017-01-04T16:20:12Z","updated_at":"2019-11-12T08:21:24Z","closed_at":"2019-11-12T08:21:17Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"RiakCache currently depends on the riak extension which hasn't been updated in years and won't build on php7.0 or 7.1.\r\n\r\n+1 to use @FabioBatSilva's https://github.com/php-riak/riak-client\r\n\r\n","performed_via_github_app":null,"score":1.0},{"url":"https://api.github.com/repos/basho/riak_repl/issues/786","repository_url":"https://api.github.com/repos/basho/riak_repl","labels_url":"https://api.github.com/repos/basho/riak_repl/issues/786/labels{/name}","comments_url":"https://api.github.com/repos/basho/riak_repl/issues/786/comments","events_url":"https://api.github.com/repos/basho/riak_repl/issues/786/events","html_url":"https://github.com/basho/riak_repl/issues/786","id":343913861,"node_id":"MDU6SXNzdWUzNDM5MTM4NjE=","number":786,"title":"Incorrect return from `handle_info` calling log spam","user":{"login":"russelldb","id":56321,"node_id":"MDQ6VXNlcjU2MzIx","avatar_url":"https://avatars0.githubusercontent.com/u/56321?v=4","gravatar_id":"","url":"https://api.github.com/users/russelldb","html_url":"https://github.com/russelldb","followers_url":"https://api.github.com/users/russelldb/followers","following_url":"https://api.github.com/users/russelldb/following{/other_user}","gists_url":"https://api.github.com/users/russelldb/gists{/gist_id}","starred_url":"https://api.github.com/users/russelldb/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/russelldb/subscriptions","organizations_url":"https://api.github.com/users/russelldb/orgs","repos_url":"https://api.github.com/users/russelldb/repos","events_url":"https://api.github.com/users/russelldb/events{/privacy}","received_events_url":"https://api.github.com/users/russelldb/received_events","type":"User","site_admin":false},"labels":[{"id":42669895,"node_id":"MDU6TGFiZWw0MjY2OTg5NQ==","url":"https://api.github.com/repos/basho/riak_repl/labels/Easy","name":"Easy","color":"5319e7","default":false,"description":null}],"state":"closed","locked":false,"assignee":{"login":"russelldb","id":56321,"node_id":"MDQ6VXNlcjU2MzIx","avatar_url":"https://avatars0.githubusercontent.com/u/56321?v=4","gravatar_id":"","url":"https://api.github.com/users/russelldb","html_url":"https://github.com/russelldb","followers_url":"https://api.github.com/users/russelldb/followers","following_url":"https://api.github.com/users/russelldb/following{/other_user}","gists_url":"https://api.github.com/users/russelldb/gists{/gist_id}","starred_url":"https://api.github.com/users/russelldb/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/russelldb/subscriptions","organizations_url":"https://api.github.com/users/russelldb/orgs","repos_url":"https://api.github.com/users/russelldb/repos","events_url":"https://api.github.com/users/russelldb/events{/privacy}","received_events_url":"https://api.github.com/users/russelldb/received_events","type":"User","site_admin":false},"assignees":[{"login":"russelldb","id":56321,"node_id":"MDQ6VXNlcjU2MzIx","avatar_url":"https://avatars0.githubusercontent.com/u/56321?v=4","gravatar_id":"","url":"https://api.github.com/users/russelldb","html_url":"https://github.com/russelldb","followers_url":"https://api.github.com/users/russelldb/followers","following_url":"https://api.github.com/users/russelldb/following{/other_user}","gists_url":"https://api.github.com/users/russelldb/gists{/gist_id}","starred_url":"https://api.github.com/users/russelldb/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/russelldb/subscriptions","organizations_url":"https://api.github.com/users/russelldb/orgs","repos_url":"https://api.github.com/users/russelldb/repos","events_url":"https://api.github.com/users/russelldb/events{/privacy}","received_events_url":"https://api.github.com/users/russelldb/received_events","type":"User","site_admin":false}],"milestone":null,"comments":2,"created_at":"2018-07-24T07:19:12Z","updated_at":"2019-10-07T11:10:09Z","closed_at":"2019-10-07T11:10:09Z","author_association":"MEMBER","active_lock_reason":null,"body":"Reported by @martinsumner https://github.com/basho/riak_repl/blob/develop/src/riak_core_service_conn.erl#L132-L134\r\n\r\nThe return is incorrect, should be `{stop, Reason, StateName}`","performed_via_github_app":null,"score":1.0}]}
0

